入围后 7 件必做事项 Checklist（复赛选手必读）

请所有入围 2025 骁龙人工智能创新应用大赛复赛 的参赛团队，务必按以下清

单逐项完成相关准备工作。

① 确认入围信息（第一时间完成）

☐ 确认入围赛道（智能手机与平板 / AIPC 创意赛 / 通用赛 / 行业赛）

☐ 确认官网显示的 团队名称、项目名称 是否准确

☐ 如发现信息有误，请立即联系赛事工作人员

② 加入官方沟通渠道

☐ 添加赛事工作人员 / 赛道小助手微信

☐ 加入对应 复赛赛道微信群

☐ 将群昵称修改为：

【赛道-赛题-团队名称-姓名】（示例：AIPC-通用赛-XXX 团队-张三）

③ 仔细阅读复赛说明文件（非常重要）

☐ 通读对应赛道的《复赛说明》全文

☐ 明确 复赛时间节点与材料提交截止时间

☐ 明确是否提供远程设备及相关使用规则

☐ 明确本赛道对提交材料的具体要求

☐ 本赛道的平台要求与设备参考说明

⚠ 未阅读规则导致材料不完整或理解偏差，责任由参赛团队自行承担。

④ 确认测试设备与平台信息

☐ 明确本项目实际使用的测试设备

☐ 明确设备对应的芯片平台（如骁龙 8 至尊版 / 骁龙 8 系列）☐ 确认设备运行环境（系统版本等）

智能手机与平板赛道需特别注意：

☐ 复赛作品需基于 骁龙 8 至尊版或骁龙 8 系列移动平台 完成端侧验证

☐ 需在材料中清楚说明设备型号与芯片平台

AIPC 赛道需特别注意：

☐ 复赛基于骁龙® X Elite 计算平台的 Windows AI PC

☐ 使用组委会提供的远程 AIPC 设备并遵守使用规则

⑤ 准备复赛 PPT（重点检查项）

☐ 梳理项目背景、应用场景与核心价值

☐ 系统架构与技术路线描述完整

AIPC 赛道团队需特别注意：

☐ 清楚说明模型运行在 CPU / GPU / NPU 上的情况

☐ 说明算力选择原因及对性能、功耗或体验的影响

☐ 如采用异构计算，说明模块划分与调度逻辑

智能手机与平板赛道团队需特别注意：

☐ 说明端侧运行方式

☐ 说明实际运行效果与稳定性

⑥ 准备项目演示视频（必交）

☐ 视频时长不超过 3 分钟

☐ 展示真实运行效果（非仅 PPT 截图）

☐ 覆盖核心功能与亮点

☐ 建议在视频中说明运行平台与测试设备信息⑦ 整理并提前完成作品提交（避免卡点）

☐ 所有材料统一打包为 一个 ZIP 文件

☐ 文件命名格式正确：赛题名称-团队名称-作品名称.zip

☐ 压缩包大小 ≤ 200MB（超出请附云盘链接）确认所有文件可正常打开、内容

完整

☐ 提前登录官网对应赛题页完成提交

☐ 确认系统显示“提交成功”

☐ 建议至少提前 24 小时完成提交，避免卡点风险

⚠ 常见问题与风险提示

• 未说明模型运行在 CPU / GPU / NPU（AIPC 赛道常见问题）

• 视频仅为 PPT 翻页，无真实运行画面

• 文件命名不规范或拆分为多个压缩包

• 截止时间临近提交失败，无法补交

📩 技术与赛事支持

如有疑问，技术问题请统一通过论坛提问：

Step1:论坛地址：https://bbs.csdn.net/forums/qualcomm?typeId=9305416

Step2:进入【AI 大赛】专区发帖

Step3:将问题链接发群内或私发小助手。

高频问答合集：

https://doc.weixin.qq.com/sheet/e3_AQ0ACwY5AGoCNcUcN17rjTEeXpO8S
1. 操作须知

①AI-PC 使用说明

每台 AI-PC 电脑供 3 个团队或个人使用,采用远程桌面(RDP)分时段共享使用.每 9

天为一个阶段，三个团队在不同时间段轮换使用.

1月10日至1月18日

1月19日至1月27日

1月28日至2月5日

第一组

04:00-12:00

12:00-20:00

20:00-04:00

第二组

12:00-20:00

20:00-04:00

04:00-12:00

第三组

20:00-04:00

04:00-12:00

12:00-20:00

上机前工作人员会向团队或个人单独发送访问地址,端口,Windows 账号及密码.

系统已设置访问时效控制,到达指定时间后,相关端口与账号将自动断开,超出时段无法

继续访问,请注意随时保存自己的工作.

注意:建议将自己的工程代码保存到个人电脑,然后通过重定向方式进行访问.windows

系统请参考小节 2-④,Mac 系统请参考小节 3-④,⑤

注意:我们给每位团队或个人的账号属于普通账号,如需要提权操作请找管理员

②禁止行为(重点)

禁止以下操作:

1. 关机、休眠或影响他人远程使用

2. 修改、添加、删除共享文件夹数据或信息

3. 更新 Windows 系统或擅自变更运行环境

4. 安装垃圾软件、病毒、木马，或进行任何恶意网络行为

③违规处理

任何不当操作造成系统异常,数据丢失或影响他人使用的,将根据情节严重程度进

行:

1.警告 2.取消远程访问资格 3.追究责任的权利

④最终说明

即视为已阅读并同意本声明内容管理方保留最终解释权及调整权2. Windows 访问远程 AI-PC 方法

①Win+R 快捷键,打开运行窗口输入: mstsc.exe 然后回车

②在弹出的窗口中选择: 显示选项

③在二级窗口中选择: 本地资源-->详细信息

④在下拉菜单中找到: 驱动器-->选择你需要使用的磁盘--->确定

备注: 此选项可以将本地硬盘重定向到 AI-PC,也就是 AI-PC 上能访问本地磁盘的资源.只有当你在操作 AI-PC 电脑的时候,重定向才会生效.其他时候不会生效

⑤点击二级菜单中的: 隐藏选项

⑥输入远程 AI-PC 的访问地址,然后选择: 连接

备注:访问地址工作人员会告知到你.⑦请输入工作人员提供的账号密码.然后选择: 确定

备注:可能你没有第一步和第二步,那么直接输入账号和密码即可.

⑧此时就可以操作 AI-PC.重定向的文件夹或盘符也显示了3. Mac 访问远程 AI-PC 方法

①Command + 空格 快捷键. 输入: App Store 然后回车

②在 App Store 商店中下载: Windows App

③下载完成之后打开软件,选择: 添加电脑

④在弹出的窗口选择: 文件夹-->重定向文件夹-->+

备注: 此选项可以将本地硬盘或文件夹重定向到 AI-PC,也就是 AP-PC 上能访问本地磁盘的

资源.⑤我们将桌面文件夹重定向到 AI-PC 远程电脑上

注意: 只有当你在操作 AI-PC 电脑的时候,重定向才会生效.其他时候不会生效

⑥输入远程 AI-PC 的访问地址,然后选择: 添加

备注:访问地址工作人员会告知到你.第二个复选框可以控制重定向文件夹的独写权限.

⑦点击我们刚添加的电脑,输入工作人员提供的账号密码.然后选择: 继续

⑧此时就可以操作 AI-PC.重定向的文件夹或盘符也显示了4. AI-PC 环境验证及测试

①在 windows 应用商店下载 python3.12(按图中步骤操作

完成上图 3 之后,会弹出商店窗口,然后点击获取就可安装 python3.12

Win+R 快捷键打开运行窗口输入: cmd 然后回车

在弹出的命令框中输入: python --version 验证是否安装好 python②拷贝案例测试文件夹和安装依赖环境

拷贝 ai-engine-direct-helper 文件夹到桌面(也可以去 C 盘目录去拷贝到桌面

xcopy C:\ai-engine-direct-helper %USERPROFILE%\Desktop\ai-engine-direct-helper\

/E /H

切换 cmd 目录到本账号桌面 ai-engine-direct-helper\samples 下

cd %USERPROFILE%\Desktop\ai-engine-direct-helper\samples

安装依赖包

pip install qai_appbuilder-2.31.0-cp312-cp312-win_amd64.whl

安装依赖包

pip install uvicorn py3-wget pydantic_settings fastapi langchain langchain_core

langchain_community sse_starlette pypdf python-pptx docx2txt openai requests wget

tqdm importlib-metadata qai-hub qai_hub_models huggingface_hub Pillow numpy

opencv-python torch torchvision torchaudio transformers diffusers

basicsr

ultralytics==8.0.193 gradio==5.30.0 -i https://mirrors.aliyun.com/pypi/simple/③环境验证及测试

完成安装后然后进行测试(注意左图无眼镜,右图有眼睛)

python python\aotgan\aotgan.py

5. 其他

使用以下文件请拷贝到自己电脑桌面上(不要剪切,也不要做修改

我们将 visual studio, android studio, git 已经安装到账户上了

xcopy C:\ai-engine-direct-helper %USERPROFILE%\Desktop\ai-engine-direct-helper\ /E /H

cd %USERPROFILE%\Desktop\ai-engine-direct-helper\samples

pip install qai_appbuilder-2.31.0-cp312-cp312-win_amd64.whl

pip install uvicorn py3-wget pydantic_settings fastapi langchain langchain_core langchain_community sse_starlette pypdf python-pptx docx2txt openai requests wget tqdm importlib-metadata qai-hub qai_hub_models huggingface_hub Pillow numpy opencv-python torch torchvision torchaudio transformers diffusers basicsr ultralytics==8.0.193 gradio==5.30.0 -i https://mirrors.aliyun.com/pypi/simple/

python python\aotgan\aotgan.py


开发者培训专区｜2025 骁龙人工智能创新应用大赛

2025-08-20

编辑：极市平台

作者：ExtremeMart

浏览：451

### 本文为2025骁龙® 人工智能创新应用大赛的开发者培训专区，主要提供本次大赛的线上直播的具体日程以及配套的讲义资料。

---

## 一、大赛直播具体日程

本次大赛共有6场线上直播，每期将于晚上8点开播

**直播间🔗：[http://live.bilibili.com/3344545](https://live.bilibili.com/3344545)**

**直播日程：**  
![](https://minio.cvmart.net/cvmart-community/images/202512/05/3/PpIHvvkcdkvwFWHiHyGD.jpg)

## 二、直播回放

[【第一期｜终端侧AI技术的前沿展望&高通技术公司AI工具链介绍】](https://www.bilibili.com/video/BV1gFyaBUEuP/?spm_id_from=333.1387.homepage.video_card.click&vd_source=e4649fd68df4b87a5edf4ef2a15b7e89)

[【第二期｜QAI AppBuilder开发指南：从入门到实践】](https://www.bilibili.com/video/BV1sb2uB7Ecd/?spm_id_from=333.1387.homepage.video_card.click&vd_source=e4649fd68df4b87a5edf4ef2a15b7e89)

[【第三期｜QAI AppBuilder移动端开发指南】](https://www.bilibili.com/video/BV1BxCxBCE2y/?vd_source=e4649fd68df4b87a5edf4ef2a15b7e89)

[【第四期｜高通AI模型开发部署工具包概述】](https://www.bilibili.com/video/BV1NjywBxE5S/?vd_source=e4649fd68df4b87a5edf4ef2a15b7e89)

[【第5期｜QAI AppBuilder实战：示例代码解读与应用演示】](https://www.bilibili.com/video/BV1UyShB4E3F/?vd_source=e4649fd68df4b87a5edf4ef2a15b7e89)

[【第6期｜基于QAI AppBuilder的AI产品介绍&入门实践、QAI AppBuilder 开发实战示例】](https://www.bilibili.com/video/BV1As2jBPELy/?vd_source=e4649fd68df4b87a5edf4ef2a15b7e89)

## 三、直播相关资料

#### 高通技术公司AI工具链介绍

> **Qualcomm AI Engine direct (Qualcomm AI Software Stack) – 文档**

1. **公开文档：**  
    [https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-50/introduction.html](https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-50/introduction.html)
    
2. **CLI 工具文档：**  
    [https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-50/tools.html](https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-50/tools.html)
    
3. **C API 文档：**  
    [https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-50/api.html](https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-50/api.html)
    
4. **API 使用和推理引擎实现示例：**  
    [https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-50/tutorials.html](https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-50/tutorials.html)
    

#### 高通AI模型开发部署工具包概述

> **QAIRT SDK分析和调试– 文档**

1. **公开文档：**[https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-50/introduction.html](https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-50/introduction.html)
    
2. **QNN HTP 配置文件与 QNN HTP Optrace 文档 ：**  
    [https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-10/htp_backend.html](https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-10/htp_backend.html)
    
3. **QAIRT Visualizer 文档：**  
    [https://docs.qualcomm.com/bundle/publicresource/topics/80-87189-1/overview.html](https://docs.qualcomm.com/bundle/publicresource/topics/80-87189-1/overview.html)
    
4. **QAIRT Python API 文档：**  
    [https://docs.qualcomm.com/bundle/publicresource/topics/80-87189-2/overview.html](https://docs.qualcomm.com/bundle/publicresource/topics/80-87189-2/overview.html)
    

**欢迎浏览与了解更多 Qualcomm AI Engine direct 中的高级用例！**

## 四、大赛报名指南

##### **[https://www.cvmart.net/community/detail/16031](https://www.cvmart.net/community/detail/16031)**

#### 报名指南｜2025骁龙AI大赛报名+作品提交一篇搞定

#### [https://www.cvmart.net/community/detail/16259](https://www.cvmart.net/community/detail/16259)

## 五、 大赛答疑

#### 1、AIPC赛道（在线文档）：

#### [https://docs.qq.com/sheet/DVVh4WkRDUmRTcWFZ](https://docs.qq.com/sheet/DVVh4WkRDUmRTcWFZ)

#### 2、智能手机和平板赛道（在线文档）：

#### [https://docs.qq.com/sheet/DVUJobW5mdERNTXhZ](https://docs.qq.com/sheet/DVUJobW5mdERNTXhZ)

#### 3、答疑提问：Qualcomm 开发者专区：

#### [https://bbs.csdn.net/forums/qualcomm?typeId=9305416](https://bbs.csdn.net/forums/qualcomm?typeId=9305416)

##### **若在【高通开发者专区】已发布问题，可同步在对应赛道的【在线文档】中写明：已在高通开发者专区提问，并附上提问链接**

|                                                                                                                                                                                                                                                                                                                                                                                                                                         |                                                                                                                                                                |                                                                                                                                      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| ![](file:///C:\Users\13632\AppData\Local\Temp\ksohtml51208\wps1.png)![](file:///C:\Users\13632\AppData\Local\Temp\ksohtml51208\wps2.png)A                      B                                     C                                                                                                                                                                                                                                  |                                                                                                                                                                |                                                                                                                                      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |                                                                                                                                                                                                                                                                                                                                                                    | ![](file:///C:\Users\13632\AppData\Local\Temp\ksohtml51208\wps3.png)                                                                                                                                                                  |
| ![](file:///C:\Users\13632\AppData\Local\Temp\ksohtml51208\wps4.jpg)                                                                                         2025骁龙人工智能创新应用大赛                                                                                                                                                                                                                                                           |                                                                                                                                                                |                                                                                                                                      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |                                                                                                                                                                                                                                                                                                                                                                    | ![](file:///C:\Users\13632\AppData\Local\Temp\ksohtml51208\wps5.png)                                                                                                                                                                  |
| 1、复赛时间：1月10日-2月5日<br><br>2 . 比 塞 形 式：<br><br>智能手机和平板赛道：本塞道聚焦智能手机与平板上的A应用创新，参塞团队基于第五代骁龙西a至尊  版、骁龙B至尊版移动平台及Qualcomm A Slack等，开发具备实际应用价值的智能应用方案。<br><br>AIPC赛道：APC赛道聚焦于基于骁龙画X Eite平台的PC端人工智能应用创新。<br><br>3、提交内容：<br><br>·参赛项目运行实拍演示视频<br><br>·编制的PPT概述<br><br>·完整的应用安装软件包与安装方法说明文档(可选项)<br><br>·项目技术文档与项目完整代码资源包(可选项)<br><br>(其中视频材料时长不超过三(3)分钟，超过三(3)分钟则仅对的三(3)分钟进行评判)。<br><br>4、提交方式：到官网对应的壅题页提交作品，官网链接；hs uc ai challeno cvmart nat2025 |                                                                                                                                                                |                                                                                                                                      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | 技术答疑相关：<br><br>1、答疑路径：<br><br>如有任何技术疑问，请到    step1:【高通开发者论坛】<br><br>[https://bbs.csdn.net/forums/qualcomm?typeld=9305416](https://bbs.csdn.net/forums/qualcomm?typeld=9305416step2:)<br><br>[step2:](https://bbs.csdn.net/forums/qualcomm?typeld=9305416step2:)【[AI](AI)大赛】专区提问，<br><br>step3:问题链接发到群里或私发给小助手，会有专人进行解答<br><br>2、答疑时间：9:00-18:00(远程设备接入及网络等问题视情况而定) | ![](file:///C:\Users\13632\AppData\Local\Temp\ksohtml51208\wps6.png)                                                                                                                                                                  |
| ![](file:///C:\Users\13632\AppData\Local\Temp\ksohtml51208\wps7.png)![](file:///C:\Users\13632\AppData\Local\Temp\ksohtml51208\wps8.png)![](file:///C:\Users\13632\AppData\Local\Temp\ksohtml51208\wps9.png)![](file:///C:\Users\13632\AppData\Local\Temp\ksohtml51208\wps10.png)                                                                                                                                                       |                                                                                                                                                                |                                                                                                                                      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 1                                                                                                                                                                                                                                                                                                                                                                                                                                       | 搭截晓龙乡平台的大赛指定PC端设备井以此作  为决赛开发设备<br><br>请问这个设备上开发可以吗?                                                                                                            |                                                                                                                                      | nttps /ishopmid cornshop hone?shopld=7032798&utm tem=Wsriends shareid2785858989908614836598B2ec73c4oc1c16417617202871<br><br>757 none none&um souroe=iosapo8ad od=share8utm campaign=t 3351397748um medium=aposhare8 ts=17617202863328x=RnAow<br><br>GBZbzbfyZaSso13WpMOx2u1L9E&xd=RnAwWBRPGDRzDAandv VH eKZIMLinZO0OTK ZBVLTa40Y2eEoCFcGOh8YrA这个京东链接里<br><br>面的都星搭载骁龙平台的设备，可以看下自己的设备是否符合要求                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 2                                                                                                                                                                                                                                                                                                                                                                                                                                       | QualcommA Stack里有没有比较实用的工具  可以帮助提升横型在手机端的推理速度?                                                                                                                 |                                                                                                                                      | Quatcomm AL Stack中工具有椒多，如果浮点楼型(FP16),可以直接在端侧运行：跑定点楼型时，我们支持INT4、INT8、INT16等多种量化  精度，精度位数越低，速度通常越快，另外，我们在端上也提供多种功耗模式(burst,powOr_saver,dafault),在不同横式下横型的运行时间会  有所不同，整体来说，Qualcomm的工具储已经比较成熟完善。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 3                                                                                                                                                                                                                                                                                                                                                                                                                                       | 用SDK的南线工具做模型量化的时候，是不足主  要就是在精度和性能之间做取舍?还有别的要注  童的吗?                                                                                                            |                                                                                                                                      | 确实，模型量化的9核心是在精度与性能之同做平衡，但在使用离线工具时，还有一些细节需要注意，SDK提供两种方式；在线(orline prepare)和   线(ofline prapare)。在线模式的优势是操作简便，不需要考滤具体平台差异，横型可以直接在学侧运行，但热行速度相对较慢，离线模式则是   在PC上先对模型进行序列化处理，把模型的算子、指令和数据结构化后再加载到设备内存(DDR)与计算单元(VTCM)执行，运行速度更快也   更利于优化不过这种方式需要根据目标平台的指令集和噪件特性进行调整。在实际使用中开发者除了关注精寝和性能的权衡外，还应注意目标   平台兼容性(确保序列化模型与硬件四配)、数据分块与优化方式(不同算子可能需要单独优化)、以及配置参数(如DDR与VTCM交互效率、  S0C设置等),总体来说，商线量化能带来显著的性能提升，但需要根据平台特性做好前期优化和验证。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 4                                                                                                                                                                                                                                                                                                                                                                                                                                       | 有转换好的qwen3的模型吗?或者转换好的横型  去哪里下载呢?                                                                                                                               |                                                                                                                                      | 关于LLM大模型，或者说LVM就足大语言和视觉模型，可以在Hugging Face (htps/uogingtace colguslcomm)或模型广场(<br><br>itos/ww aidevnome comstatamodels)上面直接下载，如Qwen-2-7B、Pi-3.5、Stable DItusin 15、ControlNet等，上面基本都能直接找到                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 5                                                                                                                                                                                                                                                                                                                                                                                                                                       | 实际开发中，从横型转换到最后在NPU上跑起<br><br>来，哪一步最容易踩坑或者最耗时间啊?有啥经  验分享吗?                                                                                                      |                                                                                                                                      | 整体来看，最容易出问题、也最花时间的环节是横型转换和量化阶段。在开发流程中，如果直接使用现成横型(杨放加从Hugging Face(  ites/iuoningfaco cogualoomm)或横型广场(htps wwaiknvhomg cnmndatalmook)下的MobilanNot,超分、去噪模型),通常比较顺利   但如果是自己训练的横型，再转成QualcommA Runtime支持的格式，这一步可能会花费较多时间，转换阶段常见的问题主要集中在算子兼容  性，不同框架导出的模型若便用了不支持的算子，就容易出现转换失败。建议在PC端先搭好Runimn环境，确保模型能正确运行，同时对照直  方文档中的算子支持列表进行检查。如果模型成功转换，接下来就总量化阶段，需要注意的足：如果进行后量化(post training quantizahion),建  议准备几百到上干条有代表性的数据进行校准，数据越多精度越稳；如果在训练阶段已插入fakse quent节点或导出了encoding信息，转换时可直  接读取量化参数，能有效避免精度损失<br><br>在调试时若发现推理结果异常或精度偏差较大，第一步应回潮检查模型转换和量化环节。高通也提供了逐居对批工具(如QL,CAIRT),可  loyor by layor地分析输出差异，快速定位问题，总体来说，模型转换、算子兼容性和量化数据准备是最容易踩坑的部分，若这几步处理得当，后  续在设备端部香通常会非常顺利。                                                                                                                                                                                                                                                                                                        |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 6                                                                                                                                                                                                                                                                                                                                                                                                                                       | SDK都有啥工具可以帮我们部署模型呀?                                                                                                                                            |                                                                                                                                      | 我们提供多种工具：低级C*+API.Python接口(QAI AppBuider),以及离此横型转换、量化优化工具，开发者可根据项目选择端侧或PC螺  部著方式                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 7                                                                                                                                                                                                                                                                                                                                                                                                                                       | Qualcomm AI Stack有没有工具让开发者吏方                                                                                                                                   |                                                                                                                                      | 有的，AI Runtime Stack SDK分为两部分：一是QNNAPI的IoW-level API(C++代码),支持Android和PC;二是Python工具QAI                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |                                                                                                                                                                                                                                                                                                                                                                    | ,                                                                                                                                                                                                                                   到 |
| 8                                                                                                                                                                                                                                                                                                                                                                                                                                       | 老师能讲讲吗，Quakomm AI Stack里那个性能  最好的API是啥?叫啥名字来着?                                                                                                                 |                                                                                                                                      | 我们主要有两种AP:1、C++API:AI Runtime的examples里有完整示例，可通过CMake编译，跨平台兼客性好；我们显提供CMakelist,也  提供Makefile,这两种根据开发者的喜好可自行选择，两种都能编择，我们是这一套AP在Androd上是用NDK来编译，在PC上我们是用Msual  Studio来编译，或者用CMakefst.CMakelist是我们推荐的，因为CMakelist可以可以胯平台，所以如果要用C+编译，我们足推荐CMakelist,  CMakeist在Linunx平台，在我们的PC平台，或者是我们的高诵的arm的PC,在X6的PC,C+就是CMakelst这三种平台是可以同时兼容而且并  不需要做过多的修改，2.PythonAPI(OAL AppBuikder):封装程度更高，用几行代码就能胞，但集成灵活性略低一般推荐使用CMake+C+                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 9                                                                                                                                                                                                                                                                                                                                                                                                                                       | 在AIPC上推理时，输入输出是否支持零拷  贝?内存是CPU的内存吗?                                                                                                                            |                                                                                                                                      | 在Motile端，我们是支持Zero Copy的Zero Copy的原理是数据可以直接从CPU注入到NPU的RPC memory(也就是CDSP内存),通过  FaslRPC通道实现高速传输，避免了多次内存拷贝。<br><br>而在AIPC上，我们采用微软的MCDM驱动体系，取代了原先的FastRPC,MCDM本身就等价于Zero Copy,它能让数据直接送入NPU驱<br><br>动，因此不再单独提Zero Copy的概念，AIPC使用的内存依然是DDR内存，NPU与内存之间通过DOR与VCTM (片上内存)交互，我们还提  供了相应的分析工具，可以观测到DDR与VCTM之间的spMI操作，帮助开发者分析性能指颈，例如当预明10毫秒的任务实际耗时80毫秒  时，可以掘比定位问题。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 10                                                                                                                                                                                                                                                                                                                                                                                                                                      | 在使用OAIAppBuider进行模型部署时，如果  模型体积较大或计算量较高，有哪些常用的优化  手段可以提升在NPU上的推理性能?                                                                                            |                                                                                                                                      | 模巴体积小，推理速度通常越快，针对大横型或计算量较离的模型，可以通过量化(Quanization)来优化性能，例如将模型从FP32精庭转换  为INT8或INT4。<br><br>这样不仅能显著减少模配体积、降低内存占用，还够或少数据传输带宽，提离在NPU上的执行效率，当然，量化也会对精庭带来一定影响，因此  通常需要进行量化感知训练(QAT)或后量化精度测优<br><br>此外，在某些复杂场景下，也可以结合CPU、GPU与NPU的算力，让不同任务在最合适的硬件单元上执行，从而获得整体最优性能。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 11                                                                                                                                                                                                                                                                                                                                                                                                                                      | 部署完模型后，怎么快速验证它足否正常运行，  性能是否达标?有没有推荐的评估方法?                                                                                                                      |                                                                                                                                      | 部害完成后，可以通过运行推理测试并保存输出结果来检查模型功能是否正确。<br><br>对于性能验证，QAI AppBuider提供了profing level参数，可在配置函数中设置性能分析等级。<br><br>将日志等圾(log levei)设为info,推理时系统会自动打印出模型加载时间、推理耗时等性能数据。  通过这些日志即可快速判断模型是否运行正常、性能是否达标。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 12                                                                                                                                                                                                                                                                                                                                                                                                                                      | 如果把CV模型落到终端设备上，有哪些常见的<br><br>坑是开发者容易踩到的?有没有一些实战经验可  以分享?                                                                                                       |                                                                                                                                      | 在储侧培语CV模型时，常见问题主要集中在两个方面：输入输出数据处理不当<br><br>a.模型前后处理通常在CPU上执行，若数据量大，可考虑使用多线程提升处理速度。<br><br>b.对于图像类任务，也可利用GPU进行图像转换、缩放等操作，以加速前后处理。<br><br>2.数据格式不匹配<br><br>a.需将原始数据(如图片)转换为模型所需的张量(Tensor)格式，并正确处理输出张量<br><br>b不同模型的输入输出格式差异较大，可参考官方GHub上的推理示例(包含二十余个横型军例),根据需要调整数按预处理和后处理逻辑。  只要解决好的后处理涉辑，并正确适配横型格式，就雄充分发挥骁龙AIPC的算力，开发出高性能、低功耗的端侧A应用，                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 13                                                                                                                                                                                                                                                                                                                                                                                                                                      | 哪一种部署方式具有更多的横型适配?                                                                                                                                              |                                                                                                                                      | 目前来看，Python格式的模型(如PyTorch.TensorFow)在生态上最为丰富。<br><br>但若要在骁龙AIPC的NPU上获得最佳性能与最低功耗，推荐使用QAIAppBuilder或OAJRT SDK部署二进制上下文格式的模型。<br><br>这两种方式中，OAI AppBuider操作更简便，同时在横型广场上已有数百个经过转换的ONN二进制上下文模型可直接下载使用。  若开发者有自训练的模型，也可根据直方文档自行转换为QNN格式进行部害                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 14                                                                                                                                                                                                                                                                                                                                                                                                                                      | 用OAI AppBuider跑大语言模型(LM)时，<br><br>怎么让内存占用更小、速度更快?有优化技巧  吗?                                                                                                     |                                                                                                                                      | 大语言模型通常参数量庞大，内存占用和推理速度主要取决于模型规模。<br><br>1优化方向包括：迭择合适规模的横型：在满足任务需求的前提下尽量使用小横型。可显著降低内存占用并提升速度；                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 15                                                                                                                                                                                                                                                                                                                                                                                                                                      | 本地跑AI模型和放在云端相比，各有什么优缺  点?                                                                                                                                      |                                                                                                                                      | 云储运行楼型的忧势在于可以支持更大、更复东的楼型，因为云梯算力更强，生成效果通常也更好。不过，这也巷味着露要通过网络传输数据，响  应速庭会受到延迟影响，而且涉及的数据隐私需要额外考虑，同时大多数情况下还需要支付云服务费用。<br><br>相比之下，本地侧运行模型的优势在于延迟低、响应快，数据和指令不必经过网络传输，隐私和安全性更高，而且运行成本几乎为零，非常适合                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 16                                                                                                                                                                                                                                                                                                                                                                                                                                      | 如果要同时跑好几个横型，QAI AppBuider是怎  么分配资源的呢?会不会卡?                                                                                                                     |                                                                                                                                      | QAJ AppBulder采用多进程架构来高效分配源井防止虚用卡顿。<br><br>其核心机制足能修将A模型加骇井运行在独立的后台服务进程中，在初始化模型时，通过指定不同的进程名称(proC_name),可以将计算密生型  的推理任务从主应用(尤其是UI线程)中剥离出去。<br><br>这种设计的忧势体现在：<br><br>1.资德隔离：每个模型或每组楼型在独立的进程中运行，内存和计算资源相互隔离。单个模型的异常不会影响主应用或其他模型的稳定性。<br><br>2.避免UI卡顿：对于图形界面应用，AI推理在后台进程中执行，U程仅负责任务分发和结果回收，从而确保了用户界面的流畅响应。  3并行处理：操作系统能够将不同的楼型盟进程测度到多个CPU核心上执行。结合骁龙芯片上A硬件(如HTP)的并行处理能力，可以实现真正高效<br><br>的多模型并行推理<br><br>因此，通过合理利用其多进程能力，QAI AppBulder可以有效管理多个模型的资源洞度，避免因A计算导致的应用卡顿                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 17                                                                                                                                                                                                                                                                                                                                                                                                                                      | 支持的模型有什么限制吗7模型大小和性能要怎  么平衡?                                                                                                                                    |                                                                                                                                      | QAJ AppBuldear对模型的支持能力主要继承自底层的Qualcomm Neural Network(QNN)SDK,<br><br>1.模型格式与算子：平台支持通过QNN工县链转换后的主流模型格式(如ONNX,TensorFlow等)。楼型的否成功运行，关键在于其内部的所有计  算算子(Operalions)是否被目标硬件后端(如HTP.CPU)所支持。<br><br>2模型大小：模型体职主要受限于目标设备的物理内存(RAM)。过大的横型可能导数加载失败或运行时内存溢出.<br><br>平衡模型大小与性能的策略：<br><br>量化(Cuantization);此为最核心的优化手段。通过将模型权重从FP³2转换为FP16或INT8等低精度格式，可显著减小横型体积、降低内存占  用井大幅提升在HTP等专用硬件上的推理速度<br><br>模型架构选型；优先选择为移动和边缘设备设计的轻量化网络架构，例如MobileNet,EfcienINat等。<br><br>-模型优化技术：可以结合使用邮枝(Pruring)、知识蒸馏(Knowladge Disilation)等先进技术，在保持精庭的同时进一步压缩横型  总的来说，平衡的关键在于应用有效的量化策略和选择合适的轻量级模型架构。                                                                                                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 18                                                                                                                                                                                                                                                                                                                                                                                                                                      | 我们项目只是用到一个小模型效果，用QAI  AppBuider上手会不会很复杂?                                                                                                                       |                                                                                                                                      | 上手不复杂OAIAppBuider对核心APi进行了高度封装，旨在简化AI模型的部署流程，对于仅使用单个小模型的项目，集成过程非常直接。  以Python接口为例，开发者仅需几步即可完成集成：<br><br>1.配置环境：通过QNNConig Confgo接口一次性完成底层库路径和运行后端的配置.<br><br>2模型初始化：通过继承QNNConted类并传入横型名称和路径，即可轻松完成模型的加载。<br><br>3.执行推理；直接调用Infaranoa方法，传入输入数据，即呵获得推理结果，<br><br>4.资源释放；对象的生命周明结束时，资源会自动被回收和释放。<br><br>相牧于直接操作底层的QNNCAPI,AppBuldar屏蔽了大量复杂的细节，使开发者能更专注于业务逻辑的实现                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 19                                                                                                                                                                                                                                                                                                                                                                                                                                      | 在Wndows和Android平台上使用，配置、接口  或者调试方式有什么区别0吗?                                                                                                                     |                                                                                                                                      | 足的，在不同平台上使用时，配置、接口和调试方式均存在差异，需要遵循备平台的开发规范。<br><br>1.配置区别：<br><br>o Wndows:主要涉及Python远行环境的依赖配置，以及确保ONN SDK相关的DLL库文件路径正确<br><br>○ Android:需要在Android Studo项目中配雪NDK,并通过CMako或ndk buid来集成C++核心库。依赖的库文件(so)需要打包到APK中。<br><br>2.接口语言与形式：<br><br>○ OAI AppBuider提供统一的C+核心接口和Python封装接口。<br><br>o在Windows平台，开发者通常会优先选择使用便滤的Python接口进行快速开发和集成，例如：<br><br>owVindows Python示例<br><br>o lamadiated=LamaDlated"lamadlated,modelpath)<br><br>o output_data=lamadilated.Inference(input_deta,input mask)<br><br>o在Androd平台，应用层(通常是Java/Kotin)需要通过JN (Java Nabve Inerface)来调用C+接口。因此，Android平台的开发主要围绕  C++APi进行。例如口：<br><br>o/AndroidC+接口调用示例<br><br>o bool resulL_init =Modelnialize(model_name”,patnto/modery<br><br>o bodl result_infer =Modelnference("model_name°,inouBuers,outputButfers,outputSlze);<br><br>3.调试方式：<br><br>oWndows:可使用Msual Studio进行C++底层代码的断点调试，或使用VS Code等工具进行Python层的网试<br><br>o Androld:需使用Android Studio配合ADB进行调试，C++原生代码的调试需要配置好NDK调试环境，并通过ogcat查看日志榆出。 |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 20                                                                                                                                                                                                                                                                                                                                                                                                                                      | 做带界面的N应用时怎么让A推理和UI界面配合  得重流畅啊?                                                                                                                                 |                                                                                                                                      | 核心策略是将A推理任务与U线程彻底分离，OAIAppBuider为此提供了完善的支持。<br><br>1.后台进程推理：利用平台的多进程架构，将模型加载和推理操作放在独立的后台服务进程中执行。这从根本上避免了计算密集型任务阻塞UI线  程<br><br>2.性能模式管理：在热行关键推理任务前，可调用SetPerProMleGlobaPerProile.BURST等接口临时提升硬件性能，缩短推理耗时。任务完成  后再释放，以平衡性能与功耗<br><br>3.高效数据传输：对于大数据量的场景(如视频流),可以使用平台提供的ShareMemory(共享内存)机制，避免在进程间进行大规模数据拷  贝，降低延迟，提升整体效率。<br><br>通过综合运用后台异步推理、性能动态管理和高效数据传输这三大策略，可以确保A功能与U界面的流畅配合，                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 21                                                                                                                                                                                                                                                                                                                                                                                                                                      | QA AppBuider主要用什么语言啊?是Python吗  还是其他的?                                                                                                                          |                                                                                                                                      | QAI AppBuilder是一个以C++为核心，同时提供Python接口的混合语言项目。<br><br>·核心实现：所有与底层QNN SDK的交互、件控制、多进程通信等核心功能，均由C++实现，以保证最高的运行效率和平台兼容性。这部分被  编译为原生动态库。<br><br>·上层接口：为了提升开发效率和易用性，项目通过pybind1技术，将C++核心功能封装成了一个易于调用的Python模块(qni_appbuikdor),  因此，开发者可以根据项目需求和平台特性灵活选择：在需要极数性能或进行底层开发的场景(如IAndroid App)下使用C++接口；在迫求快速开  发和集成的场景(如Wnows应用原型)下使用Python接口，                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 22                                                                                                                                                                                                                                                                                                                                                                                                                                      | 老师如果要把QN AppBulder做的应用上线，安  全性和稳定性方面要注意咕?                                                                                                                      |                                                                                                                                      | 将应用推向生产环境时，必须在稳定性和安全性上进行周全考虑  稳定性保障：<br><br>1.健壮的错误处理：对所有AP调用(横型加载、推理等)进行全面的返回值检查和异常捕获，设计合理的失败处理逻揖(如重试、降级、日志记  录),防止应用意外溯清。<br><br>2.严格的资源管理；确保模型资源在使用完毕后能被正确释放，防止内存泄深。特别是在服务进程中，要对资源生命质期进行精细管理。<br><br>3.进程守护与恢复：主应用应具备监控后台推理服务进程状态的能力。一旦服务进程异常退出，应有机制能自动拉起或重建服务，保证A功能的可  用性<br><br>4.全面的压力测试：在上线前，横拟高并发、长时运行等极限场景进行压力测试，确保系统在高负载下的稳定表现。  安全性加固：<br><br>1.模型保护：A横型作为核心资产，应考虚加密存储。在加载时于内存中进行解密，防止模型文件被轻易窃取和滥用。<br><br>2.安全IPC通信：保护主应用与推理服务进程之间的诵信信道，设置正确的动问权限，防止被其他恶意应用注入恶意数据或窃取推理结果<br><br>3.输入数据校验：对所有输入到横型的数据进行严格的合法性校验，防止通过构造恶意输入引发程序满或执行意外代码，防范拒绝服务(DoS)  攻击。<br><br>4.应用加固：对最修发布的应用程序包(如APK或EXE)进行代码混清和加固，提高逆向工程的门槛。                                                                                                                                                                                                                                                                                                                                           |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 23                                                                                                                                                                                                                                                                                                                                                                                                                                      | GenieAPIService调用本地NPU上的大语言模型  时，对设备有什么性的要求?内存或算力要达到  什么水平?                                                                                                    |                                                                                                                                      | 目前，只要足骁龙AI PC,都的够运行GenieAPIService调用本地NPU的大语言模型，市场上在售的晓龙AIPC都可以满足模型运行的基本条  件，至于内存需求，主要取决于想要运行的模型大小，以及系统本身在特机状态下的可用内存。一般来说，如果运行7B圾别的大语言模型，在系  统占用较任的情况下，18GB内存的设备即可满足推理需求；如果配备32GB内存，则运行会更加流畅稳定，模型加载速度也会更快。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 24                                                                                                                                                                                                                                                                                                                                                                                                                                      | 在PC端完成了模型调试，想把项目迁移到手机<br><br>上继续开发，需要改动的地方多吗?在跨平台部  署时，如果Android端和PC端的SDK版本或<br><br>驱动不同，横型精度或性能会有差异吗?                                                         |                                                                                                                                      | 这个问可以分两部分来看，首先是从PC迁移到手机际时的改动量，这与开发方式有关。<br><br>如果是传统的0+算机视觉(CV)类模型，在PC上使用C+开发且没有依赖系统特定的功能库(例女LMindows平台相关的库),那么迁移到手机端  相对容易。如果应用中便用了依赖于特定平台的接口或功够，则需要针对这些部分进行适配，如果是在PC上通过Python开发的应用，直接在手  机端运行的情况会比较少见，也可以考虑使用跨平台框架，例如Flet,这类框架能让GUI应用既能在PC上运行，也能打包成APK部害   Adroid设备上，但是否满足具体项目露求，仍需开发者自行评估。<br><br>对于使用Python实现的推理逻辑，在迁移到手机端时，通常需要将模型的前后处理逻辑和界面部分改写为C+或Android的Java实现   如果足大语言模型(LM类应用，且通过GenieAPISerVice实现的，那么迁移工作量较小，主要是把GUI客户端改为基于Android框架的版本  服务储部分可以直接在后台运行。<br><br>第二个何题关于跨平台部署时SDK或题动版本差异的影响。Ancroid和PC满的驱动确实存在差异，但如果应用是通过我们提供的标准QAIRT  SDK运行时库和QAIAppBuidar接口来实现模型加越与推理，两端是兼容的同一模型在两个平台之间迁移时，建议尽量使用相可版本的OAIRT  SDK运行时库和QAI AppBuider工具，这样能避免不必要的问题，模型精度基本不会因为版本差异而变化，性婉主要取决于不同平台NPU的算  力。                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 25                                                                                                                                                                                                                                                                                                                                                                                                                                      | 在Android端用QAI AppBuidor跑横型时，如  果模型比较大，比如超1GB的LM,怎么在内<br><br>存和加载速度之间做平衡?                                                                                       |                                                                                                                                      | 根据找们的经验，在较新的骁龙移动平台上远行3B或7B的大语言横里都是可行的，以7B模型为例，通常需要4到5GB的内存空间。对于聊天  类或文本生成类应用，这样的规模在PC威手机岸枢能流畅远行，加或裁速度和推理响应时间在多数情况下都能满足实时交互的需求，只要设备内存  充足且系统资源占用不高，就可以实现较好的横型加载和响应性能                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 26                                                                                                                                                                                                                                                                                                                                                                                                                                      | 请问在移动设备NPU上旅施多大参数量的LLM?                                                                                                                                        |                                                                                                                                      | 在最新一代的9%龙移动平台上，远行7B或8B规模的大语言横型没有问题，推理性能表现也很不指。如果横型规模进步扩大，比如13B级别，                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 27                                                                                                                                                                                                                                                                                                                                                                                                                                      | 如果遇到横型在NPU上运行出错，有什么常见的  调试方法和工具推荐吗?                                                                                                                            |                                                                                                                                      | 常见测试方法与工具：<br><br>1.启用QNN日志(设置环境变量QNN LOG LEVEL=DEBUG,输出模型加载、张量处理、推理执行日志);<br><br>2.用QNN Profiler工具，查看NPU算力占用、层执行状态，定位算子不兼寄或张量维度不匹配问题；<br><br>3.用仓库tools/convert/moded_chock py验证模型格式；<br><br>4.核对输入输出：数据类型(FP16INT8)、维度需与模型元数据一致；<br><br>5.确认SDK与驱动版本四配<br><br>6.参考samplos中的错误处理逻辑，排查资源不足、横型路径错误等问题.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 28                                                                                                                                                                                                                                                                                                                                                                                                                                      | 老师请问CV横型在NPU上运行的实时性如何7能  达到实时视频处理的帧率吗?                                                                                                                         |                                                                                                                                      | CV模型在NPU上的实时性表现优异，多数场景可满足实时视频处理，轻量CV模型(如BET分类、MobleNet适配版)驶率可达60p5+:目标  检测(YOLO轻量版)30-45ps.鸮龙PC/新代手机NPU (如XEine)支持Burst模式和多图并行优化，1080p分辨率下，主流CV任务(分<br><br>类、检测、分刻)可稳定达到30fps以上的实时标准，复杂模型经量化优化后，仍能平衡靖度与帧率，完全适配实时视频处理需求。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 29                                                                                                                                                                                                                                                                                                                                                                                                                                      | 想问实际开发中模型量化对精度有影响吗?有什  么好的平衡策略吗?                                                                                                                               |                                                                                                                                      | 量化会带来轻微精度损失，可通过以下策略平衡：<br><br>1.优先使用高通ONN量化工具(支持PTOINT8),关键层(输出层、回归层)保留FP16;<br><br>2.用覆盖业务场景的校准数据集优化量化参数，避免分布愉移导致的精度良减；<br><br>3.直接选用Huoging Face Cittps/muogingtec couakcomm)或模型广场(htps/Wwww aldavhome comidata/modals)预量化横型，已验证精庭  损失可控；<br><br>4.采用泥合量化：核心层FP16、普通层INT8,若精庄下降超阔值，可减少量化范围；<br><br>5.量化后通过准确率、mAP等指标验证，确保满足业务要求。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 30                                                                                                                                                                                                                                                                                                                                                                                                                                      | 想问一下，QA AppBuider和Android Studio是什  么关系?需要同时安装使用吗                                                                                                              |                                                                                                                                      | 两者无强制依赖，无需同时安装，是协作关系。QAI AppBuioer是高通NPU模型部看工具华，负责推理逻辑盐配、模型转换与执行：Android  Stuadio是Android开发IDE,负责U搭建、权限管理(如NPU动问权限)、APK打包Androd端开发时，可通过JN将QAI AppBuider的<br><br>c++推理库集成到Android Studo项目，或使用的者提供的Android端samples模板；纯PC开发仅需QAI AppBulder,Android Studo仅在需  开发移动端应用时便用。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 31                                                                                                                                                                                                                                                                                                                                                                                                                                      | GeniaAPIService支持哪些主流的LLM模型?  Uama,Gmma这些都可以部著吗?                                                                                                               |                                                                                                                                      | GanisAPIService支持主流开源LLM的QNN适配版，包括Llama3.132(7B/4B)、Qwan27BSSD、Phi3.5等。<br><br>1.模型格式为QNN兼容格式(含bin权重，tkenizerjson,配置文件);<br><br>2.可以从aidevhome.com下载预适配模型，                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 32                                                                                                                                                                                                                                                                                                                                                                                                                                      | 通过GeniaAPIServioe调用本地NPU运行的LLM  相比云端AP有哪些优势和劣势?延迟蜂低多<br><br>少?                                                                                                 |                                                                                                                                      | 优势：直线运行无网络依赖、数据本地留存保护隐私、无调用次数/成本限制、低延迟(7B横型单轮响应100-300ms);<br><br>劣势：模型规模受限(主流支持7B/8B)、需自行维护模型更新。<br><br>相北云端API,延迟降低60%,-80%(云端网络良好时500-1500ms,网络差时差距更大),复杂多轮对话中，本地NPU的低延迟优势更明显，但  大模型部著受限于本地硬件算力与内存。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 33                                                                                                                                                                                                                                                                                                                                                                                                                                      | 对于隐私敏感的应用场景端侧部署是不是更有  优势?性能报失可以接受吗?                                                                                                                            |                                                                                                                                      | 对于隐私教感场景(如医疗数据处理、金融遵私信息分析、个人私离交互),储侧部署的优势极为突出。依托OA AppBuloer的本地NPU推理能  力，所有数据全程在设备内处理，无需上传云端，彻庇机程网络传输中的数据泄露风险，也无需依赖第三方服务器，完全符合隐私保护法规(如  GDPR.个人信息保护法)对数据本地化的要求。从源头筑牢隐私安全防线<br><br>性能损失方面完全可接受；高通NPU的异构计算架构+QALAppBuidor的深度优化(如Burst横式、算子适配、混合量化),能最大程度抵消端  侧格B害的性能损耗，实际便用中，多数场景(如本地AI助手对话、隐私数据分类)的响应速度、推理帧率与云端差异极小，无明显感知，完全能  平衡除私安全与使用体验。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 34                                                                                                                                                                                                                                                                                                                                                                                                                                      | 请问用ONNX Runtime部看模型需要对原始模                                                                                                                                      |                                                                                                                                      | 通过ONNXRunime部 模型，不需要对原始横型做转换，使用标准的ONNX模型就可以直接部署运行。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 35                                                                                                                                                                                                                                                                                                                                                                                                                                      | 使用QualcommA Stack做端侧部害时，如果<br><br>横型精度出现下降，该从哪些环节排查?量化<br><br>算子兼容性、编译参数之间有什么调优建议?                                                                             |                                                                                                                                      | 出现精度下降时，通常需要做逐层对比，确认从哪一层开始偏差可以检查该层的量化步数(如encodng是否异常)、actiaticn的分布，以及该  层在量化转换过程中的输出情况，根据这些信息进一步定位是否是量化参数、算子支持情况或中间结果导数的问题                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 3B                                                                                                                                                                                                                                                                                                                                                                                                                                      | 能否用一个真实的横型害流程来解释OAIRT   各模块如何协同工作?例如从PyTorch模型到  最终在设备上运行，会经历哪些步骤?                                                                                             |                                                                                                                                      | 以PyTorch横型为例，流程通常是：<br><br>1)先将PyTorch模型导出为ONNX;<br><br>2)使用qait-converter转换成浮点DLC;<br><br>3)对DLC进行量化，使其能够运行在HTP上：<br><br>4)使用ONN的context/binery geneator工具将量化后的模型生成最修的Bin文件；<br><br>5)该Bin文件就是最修部署到设备端运行的模型。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 37                                                                                                                                                                                                                                                                                                                                                                                                                                      | 设备端跑多模态或个性化的GenAI应用时，延<br><br>迟有时候会比较高，有没有推荐的优化方法?比i  如模型桥分、缓存策略、或者Python API的调  用方式有没有最佳实豫?                                                                   |                                                                                                                                      | 可以先确认语言模型是否已成功从多头转换成单头；其次适当减小context length可明显提升速度；另外增加如SSD这类并行投机解码策略，也能  加速token的生成过程。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 38                                                                                                                                                                                                                                                                                                                                                                                                                                      | GenA新特性里，有没有一些针对Steble<br><br>Dilusion这类文生图楼型的特殊优化?比如推理  速度或者内存占用方面的                                                                                           |                                                                                                                                      | 对于Stable Difusion,我们会先检查模型是否也从多头成功转为单头，同时也有一些蒸馏(distilaton)策略，可减少生成步骤，从而提升推理速  度                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 39                                                                                                                                                                                                                                                                                                                                                                                                                                      | 老师，当模型部署到手机上之后，效果和在PC<br><br>上不一样，咱们的调试工具有没有什么”一键诊                                                                                                             |                                                                                                                                      | 日没有”一键诊断工具，如果遇度问题，主要还是需要逐层检查，通过层级输出对比来定位是哪一层的计算出现偏差。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 40                                                                                                                                                                                                                                                                                                                                                                                                                                      | 老师GenA在端侧的个性化微调(Fine-tuning)  具体是怎么实现的?需要的数据量和训豚时间天                                                                                                            |                                                                                                                                      | 目的还是不支持端侧R练的。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 41                                                                                                                                                                                                                                                                                                                                                                                                                                      | QAIRT 2025相比之前的版本，对开发者来说最                                                                                                                                      |                                                                                                                                      | 最明显的提升是整合了ONN和SNPE,同时新增了大量Python API,使转换、调试都更方便，现在既能支持传统模型，也能支持大模型的转换，   调试工具也比之前版本更完善。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 42                                                                                                                                                                                                                                                                                                                                                                                                                                      | QART的生态建设如何?是否有类似Hugging<br><br>Face的社区，能找到已优化井可直接在骁龙平                                                                                                         |                                                                                                                                      | 可以选用高通Hugging Face(htps/huqgingface.colqualcomm)或横型广场(ittps//www aidevhome comdataimodels)的预量化模型。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 43                                                                                                                                                                                                                                                                                                                                                                                                                                      | QAIRT支持所有主流AI框架，是不是表示<br><br>TensorFlow.PyTorch这类模型可以开箱即用?  还需要额外转换吗?                                                                                          |                                                                                                                                      | 需要经过comwerter,量化流程和context/binary generator等步骤，转换完成后才能在HTP上实际运行。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 44                                                                                                                                                                                                                                                                                                                                                                                                                                      | 新模型比如GLM4.8,YOLO13,也可以直接转                                                                                                                                      |                                                                                                                                      | 可以的，这些模型都有过部署.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 45                                                                                                                                                                                                                                                                                                                                                                                                                                      | 端侧GenAI的隐私保护是如1何实现的?模型和  数据是完全离线的吗?                                                                                                                            |                                                                                                                                      | 足完全本地化的。模型与用户数据都在设备上运行，不依致网络，也不会与云端交互，因此隐私能得到很好保障                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| ![](file:///C:\Users\13632\AppData\Local\Temp\ksohtml51208\wps11.png)![](file:///C:\Users\13632\AppData\Local\Temp\ksohtml51208\wps12.png)![](file:///C:\Users\13632\AppData\Local\Temp\ksohtml51208\wps13.png)支持por charnol和blockad quanbzation,不知道跟你所表达的groupod是不是一个概念                                                                                                                                                                |                                                                                                                                                                |                                                                                                                                      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |                                                                                                                                                                                                                                                                                                                                                                    | ![](file:///C:\Users\13632\AppData\Local\Temp\ksohtml51208\wps14.png)                                                                                                                                                                 |
| 47                                                                                                                                                                                                                                                                                                                                                                                                                                      | 做性能分析时，可视化工具游否看到海一层在                                                                                                                                           |                                                                                                                                      | 可以工具能够显示每一层的执行耗时，以及具体的内存读写情况，并以summery文件的形式呈现，方便开发者优化，                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 48                                                                                                                                                                                                                                                                                                                                                                                                                                      | 除了常规算子融合、量化外，QAIRT 2025在编                                                                                                                                      |                                                                                                                                      | 是的，可以配置不同的忧化编译迭项。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 49                                                                                                                                                                                                                                                                                                                                                                                                                                      | 目前端侧运行大语言横型(LM)是否靠谱?例<br><br>如7B横型在最新晓龙平台上的token速度、功  耗大概是什么水平?                                                                                                |                                                                                                                                      | 目前在第五代驰龙8至尊版上主要以3B和4B横型为主；在PC端，7B模型大致是20 Token's,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 50                                                                                                                                                                                                                                                                                                                                                                                                                                      | 在QAIAppBulder中部署模型时，哪些情况会  导致模型不兼吉?如何判断模型能否在NPU<br><br>上运行?                                                                                                    |                                                                                                                                      | 没有不兼容横型这种说法，理论上所有能够通过TensorFkow,PyTorch或ONNX Rurtine推理的模型，都可以转换成QNN上下文二进制格式井  运行在NPU上的。<br><br>大家查易遇到的比较难处理的问题通常不足模型能不能转换，不足模型能不能在NPU上，难点在于如何把模型量化成更小的精度的模型并且能够  保证精度不会损失过多，量化成更小的精度味着可以占用更小的内存，运行更快，但过度优化容易导致精庭损失，需要花更多时间去优化，让损  失降到合理范围                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 51                                                                                                                                                                                                                                                                                                                                                                                                                                      | 通过LangFlow调用本地模型足否会带来凝外延  退?如果延迟比较高，可以怎么优化?                                                                                                                    |                                                                                                                                      | 通过LengfFlow洞用本地模型，模型本身不会产生额外延迟，但LangFIow内部的实现有可能会号致模型的翰出不能及时显示到LEngFlow界面  上，这完全取决于LangFlow内部的实现。如果要优化的化，重多的还显从LangFlow这个开源框架的角度去优化，                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 52                                                                                                                                                                                                                                                                                                                                                                                                                                      | LangFIow构建的流程如果要嵌入本地应用(桌                                                                                                                                       |                                                                                                                                      | 通过LengFIow构速建的模型应用需要运行的话，首先需要LangFlow在后台运行。 Langf low可以把我们自己搭建的Fiow导出成基于Wieb的API  自己的应用程序可以通过这些API来调用我们在LangFkow中创建的Flow提供的功能，                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 53                                                                                                                                                                                                                                                                                                                                                                                                                                      | 多模态模型(如CLIP.Whisper)如何使用                                                                                                                                       |                                                                                                                                      | 这两个模型，我们在OAI AopBuikte GUHtub(Hb Naitnub comvagJicai angine dred helpern上正好都有相应的例子，这些例子不需要任何修改，   可以直接运行，可以去我们的GiIHub上获取代码，尝试一下.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 54                                                                                                                                                                                                                                                                                                                                                                                                                                      | 本地大横型的首token延迟一般能做到多少?是                                                                                                                                        |                                                                                                                                      | 由于我们NPU架构设计的特性，对于用户输入内容的处理非常快。而且在对话的场景中，用户一次输入的tokens不会太多，所以首tokens延迟  应该不会成为对话场景的瓶颈，                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 55                                                                                                                                                                                                                                                                                                                                                                                                                                      | 如果模型结构是自定义的(非主流渠构),在   NPU上部署会不会很困难?是否支持自定义算  子?                                                                                                               |                                                                                                                                      | 我们的QAJRT是支持自定义算子的，正如第一个问题中提到的，只要横型能够通过Tansorflow,PyTorch或ONNX Runtim推理，基本都能转换  到NPU上来运行。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |                                                                                                                                                                                                                                                                                                                                                                    | ![](file:///C:\Users\13632\AppData\Local\Temp\ksohtml51208\wps15.png)                                                                                                                                                                 |
| 56   AppBuider是否支持模型蒸馏或知识恭馏?                                                                                                                                                                                                                                                                                                                                                                                                            |                                                                                                                                                                | ![](file:///C:\Users\13632\AppData\Local\Temp\ksohtml51208\wps16.png)请注意，  QA AppBuilder是专门用来在高遥平台的NPU上加载模型并进行推理的工作，不支持训练模型或对模型进行蒸馏. |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 57                                                                                                                                                                                                                                                                                                                                                                                                                                      | GitHub示例代码里的性能benchmark靠谱吗?实                                                                                                                                   |                                                                                                                                      | 仅供参考。 Banchimarki通常在理想环境”(清空后台、敢热底好、特定系统版本)下测得，实际项目中受限于设备敢热、后台负裁和系统资源竞  争，性的通常会打折，建议预留10%-20%的余量，                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 58                                                                                                                                                                                                                                                                                                                                                                                                                                      | 模型转换的完整pipeina是怎样的?从训到部署  中间有哪些坑要注意?                                                                                                                           |                                                                                                                                      | 流程通常是：训练(PyTorch/TF)→导出(ONNX)→量化转换(ONN工具蔼)→端测害(qnn so)。<br><br>坑：最常见的是算子不支持(导数回课CPU.极其爆慢)和量化掉点(精度损失严重，需校准数据调优)。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 59                                                                                                                                                                                                                                                                                                                                                                                                                                      | 老师AppBuikder跟其他推理引擎比加TensorRT  OpenVINO)相比，在骁龙平台上的优势在                                                                                                          |                                                                                                                                      | 核心优势足硬件原生支持.TensorRT专为NVIDIAGPU设计，OpenVINO专为Intel芯片设计，它们无法调用骁龙的NPU,OAI  AppBulder/QNN是骁龙NPU的原生指令集，能效比和速度是最高的。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 60                                                                                                                                                                                                                                                                                                                                                                                                                                      | LangFIow银传统的LangChan比，在本地部署  上有咕优势?灵活性会不会差一些?                                                                                                                  |                                                                                                                                      | 优势在于可视化，降低了原型搭建和调试的门格，灵活性确实不如纯代码(LangChain),对于复杂的自定义逻辑，LangFIow可能需要手写  Custom Component(自定义组件)来实现。LangFlow中很多可视化组件其实是直接调用LangChain实现的，                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 61                                                                                                                                                                                                                                                                                                                                                                                                                                      | 遇到内存溢出或者显存不足有没有动态batch,                                                                                                                                        |                                                                                                                                      | Gradiont Chockpont是训炼技术，推理阶段用不上，推理阶段量存不足，建议使用：横型量化(INT8/NT4)、分块推理、或者限制上下文(  Context)长度，动态Batch主要提升吞吐量，对降低单次请求的峰值显存帮助有限                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 62                                                                                                                                                                                                                                                                                                                                                                                                                                      | NPU的算力跟最新的GPU比怎么样?适合跑                                                                                                                                          |                                                                                                                                      | 绝对算力任于桌面级独立显卡，但能效比(性娜功耗)远超GPU.NPU非常适台Transformer,因为其专门针对Transformor核心的大规横矩阵  乘法做了砸件级优化                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 63                                                                                                                                                                                                                                                                                                                                                                                                                                      | 边缘设备上部著这套方素，稳定性和功耗表现如                                                                                                                                          |                                                                                                                                      | NPU的功耗远低于CPU和GPU,发热较小，理论上非常适合24小时常驻运行。但实际稳定性还取决于设备的被动散热设计，如果散热不佳，   长时间满载可能会触发降频                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 64                                                                                                                                                                                                                                                                                                                                                                                                                                      | NPU的调度机制是怎样的7会不会互相抢资源?                                                                                                                                         |                                                                                                                                      | 会有资源竞争。NPU资源通常由底层驱动(ONNHexagon)管理。如果多个应用或多个模型同时请求NPU,系统会根据优先级排队或分时调  度，建议在应用层做申行化处理，避免多线程并发抢占导致延迟抖动                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 65                                                                                                                                                                                                                                                                                                                                                                                                                                      | 在QAIAppBuider上部署DDColor时，常见的<br><br>性能指颈在哪里?有哪些优先级最高的优化手  段?                                                                                                   |                                                                                                                                      | 主要的性能标颈出现在CPU的前处理与后处理环节。的处理中包合大量OpenCV操作，例如颜色空间转换、图像缩放，通道拆分合井等，这些捏  作都在CPU上热行对于高分辨率的图像会销耗大量的计算资源成为显著的性姚瓶颅，后处理同样包含了大量的CPUi计算，作如图像缩放、颜色空  间转换、数据类型转换与反归一化，这些都对CPU压力较大，<br><br>优先优化方向包括：<br><br>1.将部分前后处理迁移至NPUGPU:通过将前后处理的计算(如加缩放、颜色空间转换)集成到模型计算图中，可以利用NPU或GPU的并行计算  能力，减少CPU的负担，并避免不必要的数据拷贝；<br><br>2.用硬件加速替代常规OpenCV操作；<br><br>3.整体采用需步处理：将整个图像处理流程(包括的后处理和模型推理)放到一个独立的后台线程中执行，避免阻塞UI峨线程，从而提升应用的响应  速度和用户体验。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 66                                                                                                                                                                                                                                                                                                                                                                                                                                      | 快速部署DDColor图像上色应用时，如何优化  图像前处理和后处理以提升用户体验?                                                                                                                     |                                                                                                                                      | 1.使用吏快的图像处理库：对于图像的缩放，裁剪等操作可以考虑使用Androlo提供的Vukan或OpenGL,这些API可以利用GPU进行加速；<br><br>2.降低图像处理精度；尝试图片压缩，在不显著影响视觉效果的前提下，适当降低输入图像的分辨率；<br><br>3.提供实时进度反馈：<br><br>加较动画：在处理过程中.向用户显示一个加骏动画或进度条<br><br>分步加载：如果可能可以考虚先快速显示一个低分辨率的预览效果，然后在后台继续计算并替换为高分辨率的罹修效果。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 67                                                                                                                                                                                                                                                                                                                                                                                                                                      | 如果要让GenieChat支持多轮对话(保持上下  文),在推理与状态管理上该如何设计以保证流  畅性?                                                                                                           |                                                                                                                                      | 在工程实现上建议关注以下方面：<br><br>1.对话历史的管理(状态管理)<br><br>应用需要有一个短期记忆来存储当前的对话，我们可以在在App运行时，在内存中维护一个对话列表，如果希望即使用户关闭并重新打开App  后，对话历史依然存在，就需要将对话记录持久化存储.可以考虑使用数据库(如SQLite)或文件的形式，将对话保存在手机本地。<br><br>对话历史不能无限培长，否则会消耗过多的内存和计算资源，因此，需要设定一个记忆口，比如只保留最近的10轮或20轮对话。当对话超出这  个窗口时，最早的对话就会被速忘。<br><br>2.利用对话历史进行推理<br><br>在向AI模型发送请求时，不再仅仅发送用户当前说的这句话。而是需要将之的存储的对话历史(短期记忆)一井打包，作为背景信息发送给Al。这  样，A才能看到之前的对话，理解当的的语境，<br><br>保证流畅性的优化建议：<br><br>为了避免用户在A甲考时长时间等待，可以让A楼型以流的方式，一个词一个词地返回答案，而不是等全部答案都生成好了再一般脑地返回，这  能极大地提升用户的体验，让对话感觉更实时。(目前GenieChat已经实现了这一点)<br><br>上下文压缩(Context Prunng)三当对话历史变得很长时，全部发送给A会增加APi的调用成本和延迟，可以采用一些策略来精简上下文，比如  只发送最近的几轮对话，或者对早期的对话内容进行摘要总结。<br><br>另外，OAI AppBuider中提供的GenisAPISerics本身默认也是支持多轮对话(保持上下文)的可查看GiHub上相关文档说明。                                                                                                                                                                                                                                        |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 68                                                                                                                                                                                                                                                                                                                                                                                                                                      | CLP在QAI上推理时，Bakch Scze多大合适?  为什么Batch太大反而更慢?                                                                                                                   |                                                                                                                                      | 通常在端测NPU(如骁龙HTP)上，推荐Batch Size设置为1,或者较小的数值(如4以内),  为什么Batch太大反而慢?<br><br>这涉及端侧NPU的架构特性，与服务器斓的GPU(如NVIDIA A100)不同：<br><br>1.内存带宽瓶颈(Memory Banmicn):手机等移动设备的内存(DDR)带宽远小于服务器显存，当Batch Size增大，数据激运(从DDR到<br><br>NPU内部的高速缓存VTCM)的时间变长。如果数据传输时间超过了NPU的计算时间，就会导致计算单元闲置等待数据，从而拖慢整体速度<br><br>2.SRAM(VTCM)限制：骁龙NPU依赖内B高速向量存储器(VTCM)来极毁加速如果Batch Size过大，导致中间激活值(Acfivation)超  过了VTCM的容量，NPU就被迫将数据溢出(8pil)到较慢的DDR内存中，这会导致严重的性能下降。<br><br>3.延迟敏感；端侧应用通常追求卖时响应(Laangy),而大Batch是为了吞吐量(Throughput)。 Batch=1能保证单次操作最快完成，                                                                                                                                                                                                                                                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 69                                                                                                                                                                                                                                                                                                                                                                                                                                      | 如果想在CLIP前增加图像增强提作(如超<br><br>分),应该插在预处理的哪个环节?是否会影响  特征效果?                                                                                                       |                                                                                                                                      | 增强操作应放在图片加致之后、CLIP标准预处理(Resize/Normalize)之前，即在Image.open与“preprOess(imager之间。<br><br>对于效果的影响足一把双刃剑：<br><br>这足一把双刃剑：<br><br>1.正南影响：如果原图非常横糊(例如64x84像素),CLIP很难识别物体轮癣。此时做超分(Super-Fesoluion)恢复出细节，有助于CLIP提  取正确的语义特征。<br><br>2.负面影响：如果原图质量尚可(例如512x512),强行做妞分或增强可能会引入伪影(Artifacs)或改变图像的纹理分布。CLIP是在自然图像  上训的，过度的数字增强可能导致特征向量发生偏移(Shift),便得原本能婴的图搜不到了。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 70                                                                                                                                                                                                                                                                                                                                                                                                                                      | 如果图像库非常大(如10万张图),卖时检素                                                                                                                                          |                                                                                                                                      | 建议提前高线计算所有图像的特征，并将它们保存到单一大文件或数据库中。以常见的512维toat32特征为例，10万张面的特征约占185MB,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 71                                                                                                                                                                                                                                                                                                                                                                                                                                      | 跨平台部署时，Mac与Wndows的模型路径管  理有哪些坑?为什么Windows打包不能在Mac  上运行?                                                                                                        |                                                                                                                                      | 1.最大的误区：打包出的可执行文件”不通用<br><br>坑点：您在Windows上用Pylnstaler打包生成的exe文件(或dist文件夹),是绝对无法直接在Mac上运行的<br><br>口族因：Windows的可执行文件格式是PE(exe),而Mac是Mach 0.Pyinstaler不是Java虚拟机，它打包的是当的操作系统的原生二进制文  件.<br><br>口解决；必须在Mac系统上重新运行Pylnstller打包命令。通常的流程是：代码写一套→在Windows电脑上打个包→把代码复制至到Mac电脑  上→在Mac上再打个包<br><br>2.路径分隔符：反斜杠Vs正斜杠/<br><br>3.文件名大小写敏感(Case Sensitmity)<br><br>4.冻结路径(Frozen Path)的基准点不同<br><br>在Pylnstaller打包后，程序解压资源的临时目录机制虽然通用，但工作目录(CWD)的行为在Mac App Bundle(ap)下会很奇怪。<br><br>5.权限与写文件路径  坑点：<br><br>□ Wndows:打包后的软件通常可以险意在自己的安装目录下生成或log txt或缓存文件。<br><br>□ Mac:处于安全考虑(Gatekseper),打包好的8op内部通常是只读的，或者是签名保护的。如果你试图把缓存文件(比加代码中的  imaoe_foatures_cache pk)写回到app包的内部路径里，程序会闪退或报错Permision Denied.                                                                                                                                                                                                                                                                               |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 72                                                                                                                                                                                                                                                                                                                                                                                                                                      | 在CUP搜索基础上相增加以图榴窗，是不是<br><br>只需要将输入换成像特征?需要重新训练模型  吗?                                                                                                           |                                                                                                                                      | 是的，实现以图搜图只露用CLIP的图像编对查询图像提取特征，再与图库特征做相似度计算井排序，无需重新所训练模型，因为CLIP的核心  设计理念是图文对齐(Shared Latent Space)<br><br>这意味着：文本编码器输出的向量和图像编码器输出的向量，是在同一个数学空间里的。<br><br>□“一只猫的文字向量和”一张猫的照片向量“距离是很近的，<br><br>□同理，“一张提的照片向量“和“另一张猫的照片向量距离也是很近的。<br><br>实现以图搜图的步骤：<br><br>1.用户上传一张查询图片(Query Image).<br><br>2.使用image_encoder(不是text encoder)对这张查询图片进行推理，得到一个512维的向量query_feature.<br><br>3.使用这个query foatre去和你的图像库特征(Database Feature5)做点积计算相似度，  4.排序，返回结果                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 73                                                                                                                                                                                                                                                                                                                                                                                                                                      | 老师，想问问在NPU上部署LLM或多模态模<br><br>时，有什么选择楼吧灯模、渠构或量化萧略的  经验可以给备赛迭手参考吗?                                                                                               |                                                                                                                                      | 在本地部晋大模型时，最核心的限制通常是设备资源，因此一般忧先选择小型或轻量级横型，伊如18B以下参数规模。对于7B横型，通常需要  18GB以上内存才能稳定运行。除了模型权重本身的占用，还需要考虑上下文长度，因为更长的contoxt会显著培加推理过程中的额外内存开销.  因此在资源有限的情况下，需要同时权衡横型参数量和所露的上下文长度。<br><br>关于架构，如果足MoE(稀疏专家)结构，它对内存带宽和调度能力依赖更高，需要硬件具备足够支持才能发挥性能<br><br>在童化策路上，本地NPU上通LM时推存量化可以大际小模型体积、减少内存占用，并提升班理速度，同时精喧损失在可控范围内，像  应用主的智能启动台”使用的混元0.5B模型就是INTB量化版本<br><br>如果尽针对特定任务的场景，可以采用LoRA微调，通过在较小的基础模型上提升特定任务能力，就能在低浸源开销下获得比7B横型更好的定制  化效果，应用宝实际应用中，0.5B横型+LoRA微调后的效果已经优于一些更大横型。同时，如果有多任务需求，还可以采用“动态加就适配器的  方式，按露加载不同任务的LoRA Adapter,进一步减少内存占用。                                                                                                                                                                                                                                                                                                                                                                                                                                        |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 74                                                                                                                                                                                                                                                                                                                                                                                                                                      | 担向问实际项目落地中，把AI能力整合到传统<br><br>业务(如应用宝的分发、推荐、安全等)时，最<br><br>大的工程挑战足什么?我们比赛中也想把AI能  力嵌入已有应用，使用QA AppBulder时应该   优先考虑哪些工程点(如进程隔离、资源调度、  模型热加载等)?<br><br>A2(讲师回复整理) |                                                                                                                                      | 将AI能力融入传统业务时，最大的挑战主要来自工程层面的适配与优化<br><br>首先足硬件利用。需要合理调度CPU GPU.NPU等不同加速单元，让模型推理发挥最佳性能，高通的SDK已经做了不少NPU方向的优化  如果未来能实现多硬件协同调度，会进一步提升能力。<br><br>第二是功耗与发热，在本地没备上，如果频繁进行推理，即使是NPU也会产生较高功耗和发热因此产品层面需要减少不必要的推理任务，并依  据设备状态做动态调度，例如仅在电源充足、接入电源时执行高负载推理。<br><br>第三是数据安全与隐私即便是本地部署，也需要遵守隐私与合规要求，对于采集的数据必须做脱敏处理。对于个性化需求，可以利用用户本地数  据进行持续学习或微涸，无需上传数据到云端。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 75                                                                                                                                                                                                                                                                                                                                                                                                                                      | 应用宝的产品里，NPU推理和CPU推理足怎                                                                                                                                          |                                                                                                                                      | 应用宝针对骁龙pc适配的版本，只支持NPU推理                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 78                                                                                                                                                                                                                                                                                                                                                                                                                                      | 如果图库很大(比如10万张图),怎么优化检  索速度?要不要建索引或者用向量数据库?                                                                                                                     |                                                                                                                                      | 针对10万张级别的大规模图库检素，我们的优化核心策路显采用向量数据库配合高效的索引机制，<br><br>我们选择使用开源向量数据库LanceDB作为向量数据的存储与管理平台.LanceDB原生支持暴力搜索和近似最近邻索引两种检素模式。  在标准的PC硬件环境下，暴力搜索的耗时在毫秒圾别，这个性能水平能够满足绝大多数实时检索的应用需求.<br><br>如果面临的更大规模数据，创建索引可以显著提升搜索速度，但在构建和更新索引时会产生额外的时间开销。<br><br>因此，建议根据实际数据量、向量维度、对查询5延迟的严格要求以及可接要的索引构建耗时进行综合权衡。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 77                                                                                                                                                                                                                                                                                                                                                                                                                                      | CLIP模型的文本编码然和图像约器，在NPU                                                                                                                                         |                                                                                                                                      | CLIP可以可以分开做，也可以放到一起进行推理，看具体的use case,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 78                                                                                                                                                                                                                                                                                                                                                                                                                                      | ARM架构跟x86在AI推理上有啥本质区刷?<br><br>应用宝迁移到ARM遇到过兼容性问题吗?                                                                                                              |                                                                                                                                      | 在AI推理后向，ARM和x88集构并没有根本性的本质区，庇层设备集构(指今集、内存模型等)的复杂细节已经通过上层SDK和操作东统进  行了良好的封装和屏蔽，无论是ARM还是x86,最修的推理核心计算(三阵秦法、卷积等)都依赖于它们各自的向量化/SIMD单元(如x86的  AVX系列、ARM的NEONSVE),这些差异主要体现在性能和功耗上，而非本质的算法或功能实现上。<br><br>应用主在迁移到ARM架构时，遇到的主要兼容性挑战里中在指令集上，尽管基于ARMGMWindows提供了指令翻译来运行大部分x86应用程序，但这  种模拟井非完美。某些离性能、专用的指令集不支持，比如DAVX-512指令集，如果x86版本程序使用了这类指令集，那么在ARM平台上就需要重  新编译<br><br>因此我们应用主在迁移ARM时，使用了原生ARM64架构，对所有的代码都在APM架构下重新编译，                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 79                                                                                                                                                                                                                                                                                                                                                                                                                                      | 自定义横型转换这块。如果CLIP用了自己微凋                                                                                                                                         |                                                                                                                                      | 微调ne-tune只是针对model,转化流程不会有变化                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 80                                                                                                                                                                                                                                                                                                                                                                                                                                      | 多语言文本检索(比中英文品台),CLIP的  效果怎么样?要不要针对性优化?                                                                                                                         |                                                                                                                                      | 支持多语言需要fne tune CLIP模型，这部分需要根据use case进行调整，对于高通的工具而言，转换流程上不会有差异.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 81                                                                                                                                                                                                                                                                                                                                                                                                                                      | 图像预处理这块，Resiz9和Nomalize在NPU                                                                                                                                    |                                                                                                                                      | Resze NPU也可以做，但是速度不会特别快，建议放CPU做比较好。Nomelize NPU麦持。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| 82                                                                                                                                                                                                                                                                                                                                                                                                                                      | 老师能分享一下应用宝在内存管理上的经验吗?  怎么避免长时间运行内存泄漏?                                                                                                                          |                                                                                                                                      | 1.对于大模型，上下文在内存中会占用KV Cache,长度与内存大小直接相关，必须在性能和内存消耗之间找到最佳平衡点，设定合理的上下文长  度硬很制<br><br>可以采用滑动窗口机制，当上下文超出限制时，清理掉最归的、信息价值最低的部分。<br><br>可以引入策略将旧的聊天历史或不重要的文档压缩成摘要，用更少的stokan存德核心信息，释放原始toknn占用的KVCache.<br><br>2.对于程序中使用了多个不同模型(如驾像识别模型、文本理解模型、推荐排序模型等)的场景，应实施自动化模型生命质阴管理。  对于长时间未被调用的横型，自动将其卸载，彻底释放其占用的内存资源。将所有横型的加城和加口裁操作统一管理，避免不同模块重复加载相同模  型，实现内存共享和复用。<br><br>3.针对程序实现的内存泄通问题，在pythonf代码中，避免循环引用的代码实现。<br><br>通过手段润用gc.collec积极地回收内存。<br><br>确保系统圾备源(文件句柄、网络连接、数据库连接、线程进程句柄、C++扩展中的原生内存分配等)在使用完毕后，通过closereleaseldelete等  操作被显式释放                                                                                                                                                                                                                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
|                                                                                                                                                                                                                                                                                                                                                                                                                                         | ![](file:///C:\Users\13632\AppData\Local\Temp\ksohtml51208\wps17.png)                                                                                          |                                                                                                                                      | ![](file:///C:\Users\13632\AppData\Local\Temp\ksohtml51208\wps18.png)![](file:///C:\Users\13632\AppData\Local\Temp\ksohtml51208\wps19.png)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |                                                                                                                                                                                                                                                                                                                                                                    | ![](file:///C:\Users\13632\AppData\Local\Temp\ksohtml51208\wps20.png)                                                                                                                                                                 |
| ![](file:///C:\Users\13632\AppData\Local\Temp\ksohtml51208\wps21.jpg)                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                | ![](file:///C:\Users\13632\AppData\Local\Temp\ksohtml51208\wps22.png)                                                                |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                       |
| ![](file:///C:\Users\13632\AppData\Local\Temp\ksohtml51208\wps23.png)                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                |                                                                                                                                      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |                                                                                                                                                                                                                                                                                                                                                                    | ![](file:///C:\Users\13632\AppData\Local\Temp\ksohtml51208\wps24.png)                                                                                                                                                                 |
| ![](file:///C:\Users\13632\AppData\Local\Temp\ksohtml51208\wps25.png)![](file:///C:\Users\13632\AppData\Local\Temp\ksohtml51208\wps26.png)![](file:///C:\Users\13632\AppData\Local\Temp\ksohtml51208\wps27.png)                                                                                                                                                                                                                         |                                                                                                                                                                |                                                                                                                                      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |                                                                                                                                                                                                                                                                                                                                                                    | ![](file:///C:\Users\13632\AppData\Local\Temp\ksohtml51208\wps28.png)                                                                                                                                                                 |
| ![](file:///C:\Users\13632\AppData\Local\Temp\ksohtml51208\wps29.png)![](file:///C:\Users\13632\AppData\Local\Temp\ksohtml51208\wps30.png)                                                                                                                                                                                                                                                                                              |                                                                                                                                                                |                                                                                                                                      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |                                                                                                                                                                                                                                                                                                                                                                    | ![](file:///C:\Users\13632\AppData\Local\Temp\ksohtml51208\wps31.png)                                                                                                                                                                 |
| ![](file:///C:\Users\13632\AppData\Local\Temp\ksohtml51208\wps32.jpg)                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                |                                                                                                                                      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |                                                                                                                                                                                                                                                                                                                                                                    | ![](file:///C:\Users\13632\AppData\Local\Temp\ksohtml51208\wps33.png)                                                                                                                                                                 |
| ![](file:///C:\Users\13632\AppData\Local\Temp\ksohtml51208\wps34.jpg)                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                |                                                                                                                                      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |                                                                                                                                                                                                                                                                                                                                                                    | ![](file:///C:\Users\13632\AppData\Local\Temp\ksohtml51208\wps35.png)                                                                                                                                                                 |
| ![](file:///C:\Users\13632\AppData\Local\Temp\ksohtml51208\wps36.png)![](file:///C:\Users\13632\AppData\Local\Temp\ksohtml51208\wps37.png)                                                                                                                                                                                                                                                                                              |                                                                                                                                                                |                                                                                                                                      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |                                                                                                                                                                                                                                                                                                                                                                    | ![](file:///C:\Users\13632\AppData\Local\Temp\ksohtml51208\wps38.png)                                                                                                                                                                 |
| ![](file:///C:\Users\13632\AppData\Local\Temp\ksohtml51208\wps39.png)![](file:///C:\Users\13632\AppData\Local\Temp\ksohtml51208\wps40.png)                                                                                                                                                                                                                                                                                              |                                                                                                                                                                |                                                                                                                                      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |                                                                                                                                                                                                                                                                                                                                                                    | ![](file:///C:\Users\13632\AppData\Local\Temp\ksohtml51208\wps41.png)                                                                                                                                                                 |
| ![](file:///C:\Users\13632\AppData\Local\Temp\ksohtml51208\wps42.png)                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                |                                                                                                                                      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |                                                                                                                                                                                                                                                                                                                                                                    | ![](file:///C:\Users\13632\AppData\Local\Temp\ksohtml51208\wps43.png)                                                                                                                                                                 |
| ![](file:///C:\Users\13632\AppData\Local\Temp\ksohtml51208\wps44.jpg)                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                |                                                                                                                                      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |                                                                                                                                                                                                                                                                                                                                                                    | ![](file:///C:\Users\13632\AppData\Local\Temp\ksohtml51208\wps45.png)                                                                                                                                                                 |
| ![](file:///C:\Users\13632\AppData\Local\Temp\ksohtml51208\wps46.png)                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                |                                                                                                                                      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |                                                                                                                                                                                                                                                                                                                                                                    | ![](file:///C:\Users\13632\AppData\Local\Temp\ksohtml51208\wps47.png)                                                                                                                                                                 |
2025 骁龙人工智能创新应用大赛-AI PC 赛道复赛说明

一、复赛时间安排

复赛阶段时间：2026 年 1 月 10 日 4:00—2026 年 2 月 5 日 21:00

复赛评审时间：2026 年 2 月 6 日—2026 年 2 月 9 日

注：作品提交截止时间：2026 年 2 月 5 日 21:00（晚上 21

点），以官网提交时间为准，逾期系统将自动关闭提交入口。

二、复赛形式说明

AI PC 赛道聚焦于基于骁龙®X Elite 平台的 PC 端人工智能应用

创新。

入围团队需在初赛成果基础上，进一步完成：

•应用功能完善

•模型部署与优化

•在指定 AIPC 设备上的运行验证。

本赛道采用【远程设备接入+本地开发调试】的方式开展。

三、远程设备接入说明

（一）远程设备接入流程

Step 1、准备一台可联网的个人电脑（系统不限）

Step 2、添加小助手微信，建立一对一沟通渠道

Step 3、通过小程序抽签确认所属 AIPC 使用分组

Step 4、工作人员发送对应的 远程访问 IP、账号及密码

Step 5、按照《远程接入操作手册》完成远程连接并开始开发（二）设备使用规则说明

1、基本情况：

复赛周期：1 月 10 日–2 月 5 日（共 27 天）

参赛规模：60 支团队

设备数量：20 台 AIPC

使用方式：每 3 支团队共用 1 台 AIPC

使用规则：9×24 小时轮转使用，每队每日 8 小时

2、分组与轮换方式

使用同一台 AIPC 的 3 支团队构成一组；

每 9 天为一个阶段，三个团队在不同时间段轮换使用，保证公平。

（三）分组与轮换时间表

1月10日至1月18日 1月19日至1月27日 1月28日至2月5日

第一组 04:00-12:00

12:00-20:00

20:00-04:00

第二组 12:00-20:00

20:00-04:00

04:00-12:00

第三组 20:00-04:00

04:00-12:00

12:00-20:00

⚠请务必以分配到的组别与时间段为准，错峰使用将影响其他队伍并

可能被限制权限。

（四）重要使用规范（请务必阅读）

•务必将自己的工程代码保存到个人电脑,如远程电脑损坏数据不作

找回支持

•仅可在本团队对应时间段登录设备

•使用结束请及时退出，避免影响其他队伍•禁止跨组、跨设备使用

•禁止安装与赛事无关的软件或修改系统配置

•请提前完成模型部署和环境准备，避免占用调试时间

•因违规操作造成的数据丢失或系统异常，责任自负

四、复赛提交材料要求

参赛团队需在截止时间前提交以下材料：

（一）必交材料

1、项目演示视频（≤3 分钟）

•展示核心功能、运行效果与技术亮点

2、编制的 PPT 概述

•项目背景与应用场景

•技术架构与算法说明

•AIPC 端部署与性能表现，PPT 中需清晰、明确说明模型在 AIPC 端侧

的运行方式，至少包含以下内容：模型运行算力单元说明、算力单元

选择理由、端侧运行效果说明、异构计算使用情况（如有）

（二）选交材料

•完整的应用安装软件包与安装方法说明文档

•项目技术文档与项目完整代码资源包

（三）提交方式与格式要求

•提交格式：所有文件打包为 一个 ZIP 文件，•文件大小 ≤ 200MB，

超出大小请上传网盘，并在压缩包中附下载链接

•命名格式：赛题名称-团队名称.zip•提交方式：到官网对应的赛题页提交作品，官网链接：https://qc

ai-challenge.cvmart.net/2025

•提交路径：进入官网选择入围赛题-作品提交-完成组队（已在平台

组队可跳过）-提交作品

注：平台上创建的团队名需和初赛创意说明书团队名一致

*建议 ZIP 内部结构示例（非强制）：

/Video（演示视频）

/PPT（项目说明）

/Doc（技术文档）

/App 或 /Code（安装包或代码，可选）

五、评审规则说明

评审将从以下维度进行综合评分：

评审维度

权重

技术实现与工程能力

10%

高通平台及技术适配度

10%

创新性与前瞻性

20%

商业应用与场景价值

20%

产品设计与用户体验

20%

完整度与稳定性

20%

最终按综合得分排名，每个赛道取前 7 名晋级决赛。

*高通平台及技术适配度：至少使用 1 项以下高通技术公司及其关联公司

技术：高通技术公司及其关联公司的 AI 构架 NPU 单元、QNN SDK、QAI

AppBuilder 等；项目中需合理调用 GPU 或 NPU 资源，选手需在项目说明中详

细阐述调用方式、优化效果及其对项目创新性的具体贡献。六、注意事项

•所有材料需在规定时间内提交，逾期不予受理

•所有作品须为原创，不得侵犯第三方权益

•组委会保留对赛事规则的最终解释权

## AIPC赛道-通用赛 

## 一、赛题描述

新一代 AI PC作为“智能内容生产中心”和“知识工作助手”,具备大模型本地部署、多模态感知与实时响应能力。参赛者需基于搭载骁龙® X Elite计算平台的Windows AI PC，结合端侧AI优势与 QualcommAlStack，打造具备高交互性、高效率的创新应用。不限场景、不限行业，强调新颖性、通用性与技术挑战性。

赛题示例参考：

以下方向仅供参考，参赛作品不限于以下示例

● 新技术应用验证（如Agent框架、多模态人机交互）

● 新形态工具类产品（如AI Copilot、混合办公助手）

● 面向未来趋势的概念验证或Demo原型

## 二、比赛规则及提交说明

本次大赛分为初赛、复赛和决赛三个阶段，具体安排和要求如下：

### （一）初赛作品

比赛形式：选手基于搭载骁龙® X Elite计算平台的Windows AI PC（“大赛指定硬件设备”）进行创意构思。

**1.提交内容：**

**● 参赛创意说明书，点击链接填写相关信息：[https://doc.weixin.qq.com/forms/AH8AbAfiAAgAXMAbQY-ADoCNNheAyEJpf](https://doc.weixin.qq.com/forms/AH8AbAfiAAgAXMAbQY-ADoCNNheAyEJpf)**

**（初赛仅一个提交入口，无需在平台作品提交处提交文件，需点击链接填写，均已以此链接内容为准，其他提交方式无效）**

**● 作品的演示视频（可选项目，在【创意说明书】18项中直接提交视频链接）**

**2.格式要求**

请将**【演示视频（可选）】**上传至云盘（文件命名要求：赛题-团队名称-作品名称.zip）或者网站，将云盘链接或视频网站链接填到**参赛创意说明书18项**。

**3、提交次数：**

在报名期间提交次数不限，大赛组委会将取最后一次提交的作品，用于比赛评审；

所有作品必须在指定的截止日期前通过大赛官网指定链接提交。 

### （二）复赛作品

比赛形式：**提供远程访问的AIPC硬件环境，**需结合初赛评审的综合反馈和复赛作品要求，对作品进行更多层次的优化和完善，提交的基于搭载骁龙® X Elite计算平台的Windows AI PC（“大赛指定硬件设备”）的创新应用的完整演示说明：

**1、提交内容：**

**● 参赛项目运行实拍演示视频**

**● 编制的 PPT 概述**

**● 完整的应用安装软件包与安装方法说明文档（可选项）**

**● 项目技术文档与项目完整代码资源包（可选项）**

**(其中视频材料时长不超过三 (3) 分钟，超过三 (3) 分钟则仅对前三 (3) 分钟进行评判)。**

**2、格式要求**

请将所提交内容打包成zip压缩包，大小在200M以内

注：若文件超过200M，将内容全部上传至云盘，将云盘链接写进txt文件中，打包成zip压缩包提交。

**3、作品命名要求：**

赛题-团队名称-作品名称.zip；

**4、提交次数：**

所有作品必须在指定的截止日期前提交，仅可提交1次。

### （三）决赛作品

比赛形式：发放相关设备，完成的基于搭载骁龙® X Elite计算平台的Windows AI PC（“大赛指定硬件设备”）开发的完整人工智能创新应用及答辩方案。

**1、提交内容：**

**● 决赛答辩PPT**

**● 应用效果演示视频**

**● 完整的应用安装软件包与安装方法说明文档**

**● 项目技术文档与项目完整代码资源包（可选项）**

**（ 每位决赛选手仅限提交一 (1) 份决赛作品。一旦提交了决赛作品，提交的资料将被视为最终资料，不得再进行修改或编辑）。**

**2、答辩安排：**

决赛阶段采用线下答辩的方式，晋级决赛队伍需要提前准备。答辩现场，每支队伍面对评委有固定的陈述时间和答辩时间。评委将根据选手的方案准备情况、整体实力和现场表现进行评分，决出最终名次。

## 三、评审标准

### （一）作品提交

作品需按时提交，超过截止日期后提交的作品不予评审，同时组委会不对任何因网络故障等原因造成的作品损坏、内容缺失、提交延时等后果承担责任。

### （二）作品评审

大赛组委会指定评审专家组对作品进行评审。

1、初赛及复赛的评审内容包括但不限于：创新性、可行性、技术含量、交互体验、社会效益、商业价值、团队能力等因素，结果通过官方网页对外公布；

2、决赛采取现场评审，参赛团队完成项目路演和作品答辩后，由评审专家团队现场对作品情况进行打分，评审内容包括但不限于：作品创新性、项目可行性、技术能力、交互体验、社会效益、商业价值、作品原型、团队能力以及现场路演等。决赛结果现场公布，并举行颁奖仪式。

（以下为赛事参考标准，实际将根据赛题情况作精细调整。）

**初赛评审规则：**

团队情况与背景 20%，技术能力与落地可行性 20%，创新价值 20%，商业价值 20%，产品设计与展示表达 20%

评委依据评分排序，按各赛题取前 20 名入围第二轮比赛（复赛）。

**复赛评审规则：**

技术能力 10%，高通技术公司及其关联公司技术 10%，创新价值 20%，商业价值 20%，产品设计与用户体验 20%，作品完成度与运行效果 20%

最终排名评审标准：综合得分排序，按各赛题取前 7 名晋级决赛。

**决赛评审规则：**

项目完整性与交付质量 25%，实际运行效果与演示表现 25%，技术实现难度与创新亮点 15%，高通技术公司及其关联公司技术 10%，市场应用潜力或产业结合度 25%

根据决赛综合成绩确定获奖团队。评委会保留根据现场表现和答辩质量调整奖项归属的权利。

**说明：高通技术公司及其关联公司技术 10% 的判断标准为：至少使用 1 项以下高通技术公司及其关联公司技术：高通技术公司及其关联公司的 AI 构架 NPU 单元、QNN SDK、QAI AppBuilder 等；项目中需合理调用 GPU 或 NPU 资源，选手需在项目说明中详细阐述调用方式、优化效果及其对项目创新性的具体贡献。**

### （三）评审结论

专家组对参赛作品的评审结论一旦给出，则为最终结果，专家组将不对作品给出反馈意见。

### （四）评审专家

为确保评选工作公开、公平、公正进行，大赛组委会特邀相关技术领域的著名学者、专家、行业主管部门领导组成专家组。初赛、复赛、决赛分别邀请多名不同领域专家组成评审组，根据大赛组委会制定的评分细则开展具体评审工作。

### （五）评审方式

大赛初赛阶段、复赛阶段以“材料评审”方式为主，决赛阶段综合采用“现场路演答辩”、“作品展示”等方式组织评审工作。大赛通过“材料评审”“现场答辩”等多种方式来全方位评价参赛团队的创新能力、思维能力、表达能力等各方面的素质。

## 四、参考资料

### （一）开发者培训：[https://www.cvmart.net/community/detail/16075](https://www.cvmart.net/community/detail/16075)

从入门到进阶，从实战案例到平台深度解析，我们为开发者提供系统化培训课程。可以观看直播或回放课程，下载讲义资料，轻松掌握骁龙AI开发关键技能。 

1、线上直播｜专家面对面教学

2、课程回放｜随时随地灵活学习

3、讲义资料｜完整PPT免费下载

### （二）技术文档与工具资源：[https://www.cvmart.net/community/detail/16024](https://www.cvmart.net/community/detail/16024)

快速获取骁龙平台相关的SDK文档、接口手册、调试工具及平台能力说明，支持AI模型的高效部署与运行。 

1、WoS AI应用程序开发指南  
2、QAl AppBuilder

### （三）新闻动态与赛事公告：[https://www.cvmart.net/community/detail/16031](https://www.cvmart.net/community/detail/16031)

第一时间掌握赛事的焦点内容，从报名开始到决赛答辩，每一阶段的重要时间节点、提交要求与官方说明都将在此集中发布。  
1、官方新闻稿合集 赛程进度提醒

2、相关安排与注意事项

![](assets/高通开发/file-20260110134508705.png)本幻灯片为团队复赛递交使用的基本模板
团队可以选择提供包含音频解说的版本或常规版本
如提供含音频解说的版本，请在本页特别注明便于在评审时进行播放及聆听

幻灯片内容类型说明（请注明准确信息）：
 包含音频解说等内容
 不包含音频解说等内容


赛道名称：
AIPC 赛道（创意赛 / 通用赛 / 行业赛）
智能手机与平板赛道
团队名称：
项目名称：
是否包含音频讲解：是 / 否
请简述作品创意主题、作品在本次大赛中的思路和实现方法，以及该创意方案的亮点

程序框架（流程图或项目借鉴/参考的开源代码）作品具体使用的技术与方法
核心技术与方法
使用的 AI 技术（CV / NLP / 多模态 / Agent 等）
使用的模型或算法思路
是否基于开源模型或自研模型

核心技术与方法
使用的 AI 技术（CV / NLP / 多模态 / Agent 等）
使用的模型或算法思路
是否基于开源模型或自研模型

端侧 / 端云协同情况说明
是否支持端云协同：是 / 否
• 若支持，请说明：
• 端侧与云端各自承担的功能
• 数据交互方式
• 端侧 AI 的独立价值

部署与运行环境说明
1、已使用的骁龙平台与工具（如有）
请根据项目实际情况说明所使用的平台与环境：
骁龙® X Elite 平台（AIPC 赛道）
骁龙移动平台（智能手机 / 平板赛道）
端侧部署方式
推理或运行环境说明

可结合 Qualcomm AI Stack 进行模型部署、加速或优化，但不作强制要求。 
2、其他软硬件平台说明（如有）
可补充说明实际使用的其他软硬件平台
仅需说明客观使用情况，无需进行性能或平台对比

一、运行环境说明
测试平台： 骁龙® X Elite
运行方式： 端侧本地运行 / 端云协同（如有）
二、典型内存与性能截图（示例）
（请插入 1–2 张方案运行过程中的系统资源监控截图）



三、算力使用说明（典型状态）
CPU： 主要用于控制逻辑 / 数据预处理（约 XX%）
GPU： 用于并行计算 / 图像处理（如有，约 XX%）
NPU： 用于核心模型推理（如有，约 XX%）四、内存占用与稳定性说明
典型内存占用： 约 XX
运行稳定性： 连续运行正常，无异常情况

Demo 功能点说明
核心操作流程
性能与运行表现
• 推理延迟 / 响应时间
• 稳定性说明
• CPU / GPU / NPU 使用情况（如有）

不强制要求跑分，重点在 与应用场景匹配的合理性。

可进一步强调您的作品方案的优势与特点等，有助于大赛评委放了解作品特色的相关信息。
可简述参赛经验，如遇到的困难及解决方法。
关键日志或截图
模型结构说明
工作流或模块调用关系

视频要求：
录制应用在比赛设备上运行的相关界面和展示, 视频长度不超过3分钟
视频命名格式为：赛道_团队名称
视频上传链接：竞赛官网提交作品页面

**智会 Pro 产品白皮书**

面向未来的智能会议助手与 TOPGO 团队能力全景

1. # 引言
    

产品名称

**智会 Pro**

版本

**2.0 企业版**

发布日期

**2026-01**

本白皮书旨在全面阐述智会 Pro 的核心价值与技术架构，面向寻求会议智能化转型的企业客户、寻求技术落地的合作伙伴以及行业评审专家。随着生成式 AI 技术的爆发，传统会议场景正面临效率与体验的双重重构。[智会 Pro](https://home/user/workspace/resources/webpages/webpage_智会PRO路演稿.md) 应运而生，定位为一款聚焦多模态交互与端云协同的智能会议助手。我们致力于通过 “智能记录 - 本地控制 - 知识管理” 的闭环体系，解决会议中的信息孤岛与操作繁琐问题，将每一次会议转化为可沉淀的组织资产。

2. # TOPGO 团队与能力版图
    

TOPGO 团队不仅仅是技术的探索者，更是赛场上的常胜将军。在过去的两年中，我们持续征战于顶尖科技公司与国家级技术平台，从火山引擎到百度，从英特尔到腾讯云，我们以平均每月一次的获奖频率，完成了从算法创新到工程落地的 “赛场级工程化验证”。这一连串的荣誉不仅验证了我们技术的领先性，更彰显了团队在极限压力下的交付能力与稳定性。这些密集的获奖节点并非孤立存在，它们共同编织了 TOPGO 团队在 **硬件适配、算法创新、应用开发、数据鲁棒性与产业融合** 五大维度的全景能力版图。

### 2024

**火山引擎 AI 创造者大赛**

2 项二等奖 | 2024-09

**火山引擎 “程序员智能搭子”**

2 项三等奖 | 2024-10

**百度 1024 程序员大赛**

三等奖 | 2024-10

**无限易征文大赛**

鹏程万里奖 | 2024-10

### 2025-2026

**开放原子开源基金会大赛**

三等奖 | 2025-03

**数字中国创新大赛**

优胜奖 | 2025-05

**广东省工信厅主题征集**

优秀奖 | 2025-07

**英特尔人工智能大赛**

优胜奖 | 2025-08

**腾讯云 Agent 挑战赛**

优胜奖 | 2025-08

**Vibe Coze！AI 挑战赛**

优胜奖 | 2025-12

**智能 Agent 创新大赛**

开源奖 | 2025-12

**高德空间智能开发者大赛**

优胜奖 + 特别奖 | 2026-01

1. ### 获奖清单详情
    

|比赛名称|奖项等级|时间|
|---|---|---|
|火山引擎 AI 创造者大赛|2 项二等奖|2024-09|
|火山引擎 “程序员智能搭子” 比赛|2 项三等奖|2024-10|
|百度 1024 程序员大赛|三等奖|2024-10|
|无限易征文大赛|鹏程万里奖|2024-10|
|开放原子开源基金会科研创新大赛|三等奖|2025-03|
|数字中国创新大赛（模型 BadCase 挑战）|优胜奖|2025-05|
|广东省工信厅 “法治当先行” 主题征集|优秀奖|2025-07|
|英特尔人工智能大赛（企业赛道）|优胜奖|2025-08|
|腾讯云黑客松 Agent 应用创新挑战赛|优胜奖|2025-08|
|Vibe Coze！扣子 AI 挑战赛 2025|优胜奖|2025-12|
|智能 Agent 创新大赛 PC GUI|开源奖|2025-12|
|高德空间智能开发者大赛|优胜奖 + 华为鸿蒙特别奖|2026-01|

3. # 愿景：技术民主化与迷你影子军团
    

![](https://www.anygen.io/space/api/box/stream/download/asynccode/?code=MTIzNzNhZjEwOTIzMTI5ZGVmZjViMjhhZDNhYjMyMThfQmtBcjlWNW1nQmlsZ0lQVGlZV21kTEVrVmdtV2dkRUVfVG9rZW46TEVRRmJuNFVmb2RsdlF4VWJUNmxNTEFFZzZVXzE3NjgwMjcyMjM6MTc2ODAzMDgyM19WNA)

我们的愿景是实现 “技术民主化”。我们相信，顶尖的 AI 技术不应仅停留在实验室或大型机房，而应成为人人可用的简单工具。TOPGO 致力于将冠军级的能力封装在简洁的界面之后，让每一位用户都能轻松驾驭。

为此，我们提出了 “迷你影子军团” 的工作模式。在用户视角下，智会 Pro 只是一个简单的交互界面；但在后端，它是由多个专业智能体组成的 “影子军团” 在协同工作。用户只需发出自然语言指令，系统便会自动调度语音识别 Agent、设备控制 Agent 和知识库 Agent 进行复杂的任务拆解与执行，最终直接呈现用户想要的结果。

4. # 问题与洞察：会议痛点与闭环设计
    

![](https://www.anygen.io/space/api/box/stream/download/asynccode/?code=NzQ5YTA3M2E0N2ZhZWNjZGYxN2E1NDI4OWExNzZmZjJfbU1zRGdSb2RGUDB1RW9kTVc1UmJzbE5rUHg2QzVCWkVfVG9rZW46QWF0SGJIRnM3b0tRS0N4a3B1NmxyMThJZ2xnXzE3NjgwMjcyMjM6MTc2ODAzMDgyM19WNA)

通过对数百场传统会议的深度洞察，我们归纳出三大核心痛点：

### 发言人身份模糊

在多人讨论或跨国会议中，难以精准区分 “谁说了什么”，导致纪要混乱。

### 设备操作繁琐

频繁切换投屏、调节音量、截屏记录，打断会议节奏，分散参会者注意力。

### 信息管理分散

会议录音、PPT、截图和文字笔记散落在不同设备与软件中，难以形成完整知识资产。

针对上述痛点，智会 Pro 构建了 “语音识别 → 设备操作 → 知识管理” 的完整闭环。我们采用分层架构设计，将会议全流程划分为 “会前准备、会中协同、会后沉淀” 三个阶段，通过事件驱动机制，让数据在各阶段间自动流转，彻底打破信息孤岛。

5. # 技术架构与端云协同
    

![](https://www.anygen.io/space/api/box/stream/download/asynccode/?code=YzljMzk2YTcyNTFhZjNjZjZlZDJkZWJlNzM5ZTQxMDZfbDhoVnZjVFBwQlhzNEVINlREVHRrSlphUlF0TDZtcnpfVG9rZW46SUM1a2IyaTQwb3BFTDV4QTN2bGx0OG1jZ0JoXzE3NjgwMjcyMjM6MTc2ODAzMDgyM19WNA)

## 模型栈 (Model Stack)

智会 Pro 集成了业界领先的生成式 AI 模型，确保在不同任务下都能获得最佳效果：

- **KIMI**：用于长文本处理与上下文理解。
    
- **豆包 1.5 Pro 32k**：提供强大的通用语言处理能力与长窗口支持。
    
- **豆包 1.5 Pro 视觉理解**：赋予系统 “看” 懂屏幕与图像内容的能力。
    

## 端侧组件 (Edge Components)

基于英特尔 AI PC 工具箱，我们构建了高效的本地组件：

- **speech_to_text**：本地/云端混合语音转写。
    
- **screenshot_embedding**：智能截图向量化处理。
    
- **OCR & 设备控制**：实时文字提取与硬件层级操作。
    

**为何选择端云协同？** 会议场景对实时性和隐私性要求极高。端侧算力确保了设备控制的毫秒级响应和敏感数据的本地化处理，避免了网络延迟带来的卡顿；而云端大模型则提供了强大的推理与总结能力。这种架构在性能、稳定性和隐私保护之间取得了完美的平衡。

6. # 核心亮点：多模态闭环能力
    

### 语音转文字自动化

依托实时语音识别技术，将会议语音流转化为结构化文本。我们实现了高达 98% 的识别准确率，并支持多语言混合识别，确保不错过任何关键信息。

### 智能截图嵌入

系统能自动感知屏幕上的重要内容变化，自动截图并利用 OCR 技术提取关键数据，将视觉信息无缝嵌入到文字记录中，还原会议现场。

### 知识库智能关联

转录内容会实时与企业内部知识库进行双向链接匹配。系统自动识别关键词，建立跨会议的知识图谱，让新旧知识瞬间融会贯通。

**🔄 闭环流程：语音输入 → 文本转化 → 视觉增强 → 知识沉淀**

7. # 核心亮点：本地设备多模态操作
    

### 智能截图插件

一键捕获屏幕，自动识别窗口重点，支持即时标注与分享。

### OCR 识别引擎

实时提取图像文本，支持多语言，将图片转化为可检索数据。

### 精准设备控制

支持音量、亮度、投屏等 10+ 项硬件控制，解放双手。

**本地控制的优势**：通过调用 [volume_adjustor](https://home/user/workspace/resources/webpages/webpage_智会PRO路演稿.md) 和 brightness_adjustor 等本地函数，我们将操作延迟降低至毫秒级，远快于云端回调方案。同时，屏幕画面和本地文件操作完全在设备端完成，无需上传云端，彻底消除了用户对隐私泄露的顾虑，让会议辅助更加安全、跟手。

8. # 核心亮点：智能内容整理与输出
    

## 文档自动生成

会议结束后，智会 Pro 不仅仅是输出一段录音。它会调用智能插件，自动生成结构化的 PDF 或 Word 文档。系统会按议题、决策点和行动项（Action Items）对内容进行分类整理，确保每一分钟的会议都有价值产出。

## 标准化纪要输出

我们提供标准化的会议纪要模板，自动填充以下要素：

- **时间戳索引**：点击文字即可跳转对应录音。
    
- **发言人区分**：清晰标注每位参会者的观点。
    
- **关键结论卡片**：AI 提炼的核心决策与待办事项。
    

**💡 场景示例**：在一场时长 2 小时的产品评审会后，智会 Pro 在 1 分钟内生成了一份包含 5 个核心议题、12 张关键 PPT 截图、3 项最终决策的精美 PDF 报告，并直接分发给所有与会者。

9. # 实现机制：算力整合与端侧工作流
    

![](https://www.anygen.io/space/api/box/stream/download/asynccode/?code=Yjg5NjE1ZjA1OTgxYmJlNWFlYTBlN2I2ZDk5ZDA3YzFfUUdaTmVCVkY5dVZOVGI2T3pvQVRzd3BCbDNiZXFqS0dfVG9rZW46QlJhM2JQYUhGb1dPem94V2tFd2xjMWRiZ3ViXzE3NjgwMjcyMjM6MTc2ODAzMDgyM19WNA)

为了在有限的硬件资源上实现上述功能，我们对算力进行了深度整合。通过智能分配计算资源，我们在保证语音转录与截图识别并行处理的同时，成功减少了 40% 的 CPU 占用，确保前台办公软件流畅运行。

在端侧工作流方面，我们定义了一系列标准化的端侧函数（Function Call）：这种设计体现了我们在性能、稳定性与可维护性之间的权衡艺术：用端侧算力换取极速响应，用云端大模型换取深度智能，用模块化函数换取系统的高可扩展性。

- **clipboard_text_copy**：将关键信息复制到剪贴板并准备上传。
    
- **image_input**：在光标处直接插入云端生成的图片。
    
- **knowledge_base_query**：不联网的情况下检索本地私有知识库。
    

10. # 应用场景与效果
    

### 🌏 跨国视频会议

**挑战**：多语言环境、口音重、记录难。 **价值**：实时多语言转写与翻译，准确识别每位发言人。 **结果**：消除了语言障碍，沟通效率提升 50% 以上。

### 🎓 学术研讨会

**挑战**：专业术语多、知识密度大。 **价值**：精准记录专家观点，自动提炼关键知识点。 **结果**：为后续研究提供了结构清晰、内容详实的参考资料。

### 🏢 企业内部会议

**挑战**：信息分散、跟进困难。 **价值**：自动截图归档，与企业知识库实时联动。 **结果**：形成了完整的会议资产闭环，任务跟进率显著提高。

11. # 可靠性背书：赛场级验证与产品化
    

![](https://www.anygen.io/space/api/box/stream/download/asynccode/?code=NTg4NGZiM2QzYjAyYjMyNmU0ZTJhNTI3OTIyMzQwNWZfdVoyd2tnWHRQN2MxSDhxeDhTTWpRMU9mSmpwclhmRTZfVG9rZW46TXduTGJMNUNRb2lEazh4aWJuVWx0UWg5Z0RkXzE3NjgwMjcyMjM6MTc2ODAzMDgyM19WNA)

智会 Pro 不是温室里的花朵，而是经历了残酷赛场洗礼的战士。我们的可靠性建立在 “赛场级验证” 的基础之上。从火山引擎的创造者大赛到英特尔的企业赛道优胜，每一次获奖都是对我们技术鲁棒性、泛化能力和高强度并发处理能力的极限测试。

### 鲁棒性验证

通过数字中国 “BadCase 挑战赛” 验证，系统在噪声干扰、模糊图像等极端环境下依然保持高可用性。

### 规模化交付

依托百余个 Agent 的开发经验，我们将赛题解决方案标准化为可交付的软件产品，确保企业级部署的稳定性。

12. # 合作模式与落地路径
    

TOPGO 提供灵活多样的合作模式，从标准工具包到深度定制开发，全方位满足客户需求。我们的落地路径清晰明确：

**需求沟通**

深入调研会议场景与痛点，明确功能边界与定制需求。

**方案与试点**

提供 POC 原型进行小范围试点，验证效果与流程打通。

**交付与评估**

完成软件部署、知识库对接与员工培训，进行全员推广。

**长期运营**

持续更新 AI 模型，提供数据运维支持，确保系统越用越聪明。

**⚖️ 授权与合规**：我们严格遵守数据安全法规。支持私有化部署（On-Premise）与混合云模式。企业数据完全掌控在客户手中，账号权限分级管理，确保信息安全无忧。

**共建智能会议新生态**

智会 Pro 不仅仅是一个工具，更是企业数字化转型的加速器。我们诚挚邀请您预约演示，共同探索 “技术民主化” 带来的无限可能。

![](https://www.anygen.io/space/api/box/stream/download/asynccode/?code=MTgzMGM5N2MxYmYyNDMzMTc2ZGE5MjRhNThlYmRhNDlfZDRxV01rNG9uc2pMeHNGQ1RpUDhqMWlMOTNsZURzZXBfVG9rZW46RHdjbWJ3bGE5b2dxNlB4NXdMZ2xHbFNrZ0FkXzE3NjgwMjcyMjM6MTc2ODAzMDgyM19WNA)TOPGO智能体军团：从冠军赛场到您手中的“瑞士军刀”
一、 我们是谁：一群拿奖拿到手软的全能型技术先锋
比赛名称	奖项等级	
2024年9月 火山引擎AI创造者大赛	2项二等奖
2024年10月 火山引擎“程序员智能搭子”比赛	2项三等奖
2024年10月 百度1024程序员大赛	三等奖
2024年10月 无限易征文大赛	鹏程万里奖
2025年3月 开放原子开源基金会科研创新大赛	三等奖
2025年5月 数字中国创新大赛数据要素赛道（盲区突围：模型BadCase极限挑战赛）	优胜奖
2025年7月 广东省工业和信息化厅“法治当先行 粤造粤先进”主题征集活动	优秀奖
2025年8月 英特尔人工智能大赛（企业赛道）智会PRO 优胜奖 2025年8月 腾讯云黑客松Agent应用创新挑战赛 优胜奖 2025年12月 Vibe Coze！扣子 AI 挑战赛2025 优胜奖 2025年12月 智能 Agent 创新大赛PC GUI开源奖2026年1月 高德空间智能开发者大赛 优胜奖+华为鸿蒙特别奖 我们是TOPGO智能体军团，一群在AI赛场中以实战证明价值的“多面手”冠军。在过去一年中，我们的足迹遍布全球顶尖科技公司与国家级赛事平台，在一个个高难度挑战中，证明了我们不仅“能打”，而且“全面”。
硬核基建专家：我们在英特尔AIPC应用创新大赛中，将智能体与边缘计算硬件深度融合，解决了在资源受限环境下稳定运行的难题。
创意与落地大师：在字节跳动火山引擎&扣子AI应用开发大赛中，我们以极具创意的智能体应用脱颖而出，证明了技术想象力与产品化能力的完美结合。
数据与鲁棒性强者：在数字中国创新大赛等国家级数据赛事中，我们的模型在复杂、嘈杂的真实数据环境中展现了卓越的鲁棒性和泛化能力。
产业智能化尖兵：在工信部智能制造大赛和华为鸿蒙高德智能空间大赛中，我们将多智能体协同技术应用于工业与物联网场景，实现了从感知到决策的闭环。
我们不是在单一赛道追求极限的“偏科生”，而是在硬件适配、算法创新、应用开发、数据攻坚、产业融合等全方位战场经受锤炼的“全能型选手”。​ 从阿里巴巴的决赛路演舞台到国家级的竞技场，我们拿回的十余个重磅奖项，共同拼出了一幅AI综合能力版图。
二、 我们的愿景：将冠军能力，锻造成人人可用的“简单工具”
在征服了无数技术山峰后，我们开始思考一个更根本的问题：这些凝聚了顶尖智慧的能力，如何才能走出赛场，真正照亮每一个普通人的工作与生活？
我们看到了那个在老家寻找兼职机会的大学生，那位希望利用碎片时间创造价值的宝妈，以及无数被重复性劳动所困、渴望解放创造力的上班族。他们需要的不是一个需要博士学历才能理解的“AI神话”，而是一把趁手、锋利、能立刻解决实际问题的“瑞士军刀”。
因此，TOPGO的使命是“技术民主化”：我们将从多智能体协同、GUI自动化、跨平台应用开发、数据智能运营等大赛中锤炼出的尖端能力，彻底打碎、重组、封装。​ 目标是打造一系列极度简单、却蕴藏着冠军实力的智能体工具，让每个人都能拥有一个属于自己的 “迷你影子军团”​ 。
这个“影子”能听懂你最朴素的指令，并调动背后的“冠军智能体”们协同工作，将复杂的技术流程化为无形，只将你想要的结果呈现在面前。
三、 案例展示：当冠军智能体为你打工
案例一：大学生的“创收影子” - 从想法到上线，只需一句话
用户与痛点：大学生“小张”，想为自建的读书社群开发一个打卡小程序，用于活跃氛围并赚取少许会员费。但他不懂编程，预算有限，外包开发周期长、成本高。
智能体如何工作：
需求理解体：小张只需说出：“我想做一个打卡小程序，用户可以每日发布读书笔记和打卡，并能互相点赞评论，最后能自动统计积分。” 智能体通过对话厘清细节（如积分规则、UI风格）。
架构规划体：基于在WEB与小程序大赛中积累的组件库，自动生成前端页面流、后端数据表结构和云函数逻辑。
代码生成与部署体：调用在阿里巴巴GUI自动化大赛中验证的代码生成能力，自动输出小程序源码，并一键部署至云开发环境。
效果：将一个原本需要2-3周、花费数万元的外包项目，缩短为10分钟对话和1小时的自动部署过程。​ 小张的“创收工具”在一天内从想法变为现实。
案例二：宝妈的“内容分身” - 一键分发，全网开花
用户与痛点：宝妈“李姐”，是育儿经验分享者，手头有精心整理的图文干货，但手动分发到小红书、抖音、公众号等十个平台，需要反复复制粘贴、调整格式，耗时耗力，且容易出错。
智能体如何工作：
内容解析体：李姐上传一份Word文档。智能体运用在文档结构化处理中的技术，自动提取核心观点、亮点句子和配图建议。
多平台适配体：调用在字节跳动火山引擎大赛中积累的跨平台内容理解模型，自动为小红书生成“好物分享”笔记体、为抖音生成短视频脚本大纲、为公众号生成深度文章。
自动化运营体：集成在自动化运营赛事中打磨的流程引擎，在指定时间，模拟人工操作，将内容自动发布至各平台（需提供账号授权），并监控初始数据反馈。
效果：将一次多平台内容发布的操作，从3-4小时的人工劳动，压缩为5分钟的准备和全自动执行。​ 让李姐能专注于核心的内容创作，而非重复的机械劳动。
案例三：上班族的“效率副驾” - 告别重复，聚焦核心
用户与痛点：市场专员“王先生”，每周需要从多个系统导出销售数据，在Excel中手工合并、清洗、制作固定格式的周报图表，过程枯燥且容易出错。
智能体如何工作：
数据抓取体：王先生授权后，智能体像一支训练有素的特种部队，“采集智能体”​ 自动登录CRM、ERP等系统，定位并抓取指定数据表。
处理与融合体：“清洗智能体”​ 依据预设规则（如去重、格式化），自动处理原始数据；“分析智能体”​ 执行预置的Excel公式和透视表操作，完成计算。
报告生成体：“呈现智能体”​ 将分析结果，自动填入预设的PPT模板，生成包含图表、结论的完整周报文档，并通过邮件发送给领导。
效果：将每周耗费半天（约4小时）的重复性数据处理工作，降低为全程无人值守的10分钟自动流程，错误率趋近于零。​ 王先生只需在最终报告上做最终审阅即可。
结语：把时间，还给生活本身
TOPGO智能体军团的故事，始于顶尖的赛场，但不止于赛场。我们正将那份赢得冠军的技术荣耀与工程能力，转化为每个人触手可及的简单力量。
我们的目标，是让你“说话”就能创造产品，“动念”就能分发内容，“授权”就能自动化工作。让AI帮忙干活、赚钱、学习，然后，把省下来的时间，真正还给生活本身。
这，就是我们从“冠军智能体”到“你的智能体”所走的路。 润色优化做成PPT