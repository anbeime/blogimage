åˆ›å»ºä¸€ä¸ªâ€œè®ºæ–‡æ´å¯Ÿä¸“å®¶â€æ™ºèƒ½ä½“ï¼Œå®ƒèƒ½å¤Ÿï¼š

1. è‡ªåŠ¨è¿½è¸ªæœ€æ–°è®ºæ–‡ï¼ˆé€šè¿‡ArXivç­‰å¼€æ”¾å¹³å°ï¼‰
    
2. æ ¹æ®ä¸»é¢˜è¿›è¡Œæ£€ç´¢å’Œæº¯æºï¼ˆæŸ¥æ‰¾ç›¸å…³å·¥ä½œå’Œå¼•ç”¨ï¼‰
    
3. è‡ªåŠ¨æ’ç‰ˆå¹¶è°ƒèŠ‚æ ¼å¼ï¼ˆç”ŸæˆæŠ¥å‘Šï¼Œæ”¯æŒæ ¼å¼ä¿®æ”¹ï¼‰
    

æˆ‘ä»¬å°†ä½¿ç”¨Claude Agent SDKï¼Œå¹¶ç»“åˆä¸€äº›å­¦æœ¯æœç´¢å’Œæ–‡çŒ®ç®¡ç†å·¥å…·ã€‚

æ­¥éª¤æ¦‚è¿°ï¼š

1. ç¯å¢ƒè®¾ç½®ï¼šå®‰è£…å¿…è¦çš„åº“ï¼Œå¦‚arxivã€requestsã€beautifulsoup4ç­‰ã€‚
    
2. æ„å»ºå·¥å…·ï¼šå®šä¹‰æ£€ç´¢è®ºæ–‡ã€åˆ†æè®ºæ–‡ã€ç”ŸæˆæŠ¥å‘Šçš„å·¥å…·ã€‚
    
3. è®¾è®¡æ™ºèƒ½ä½“å¾ªç¯ï¼šè®©Claudeèƒ½å¤Ÿæ ¹æ®ç”¨æˆ·éœ€æ±‚è‡ªåŠ¨æ‰§è¡Œä»»åŠ¡ã€‚
    
4. æŠ¥å‘Šç”Ÿæˆä¸æ’ç‰ˆï¼šä½¿ç”¨Markdownæˆ–LaTeXç”ŸæˆæŠ¥å‘Šï¼Œå¹¶å…è®¸ç”¨æˆ·è°ƒèŠ‚æ ¼å¼ã€‚
    

ç”±äºè¿™æ˜¯ä¸€ä¸ªå¤æ‚çš„é¡¹ç›®ï¼Œæˆ‘ä»¬å°†åˆ†æ­¥éª¤å®ç°ã€‚

ç¬¬ä¸€æ­¥ï¼šç¯å¢ƒè®¾ç½®

å®‰è£…å¿…è¦çš„PythonåŒ…ã€‚

ç¬¬äºŒæ­¥ï¼šæ„å»ºå·¥å…·

æˆ‘ä»¬å°†æ„å»ºä»¥ä¸‹å·¥å…·ï¼š

- æœç´¢è®ºæ–‡å·¥å…·ï¼ˆä½¿ç”¨ArXiv APIï¼‰
    
- ä¸‹è½½è®ºæ–‡å·¥å…·ï¼ˆè·å–PDFï¼‰
    
- è§£æè®ºæ–‡å·¥å…·ï¼ˆä»PDFä¸­æå–æ–‡æœ¬å’Œä¿¡æ¯ï¼‰
    
- ç”ŸæˆæŠ¥å‘Šå·¥å…·ï¼ˆå°†ä¿¡æ¯æ•´ç†æˆæŠ¥å‘Šï¼‰
    

ç¬¬ä¸‰æ­¥ï¼šè®¾è®¡æ™ºèƒ½ä½“

ä½¿ç”¨Claude Agent SDKï¼Œè®©Claudeèƒ½å¤Ÿä½¿ç”¨è¿™äº›å·¥å…·ï¼Œå¹¶æ ¹æ®ç”¨æˆ·éœ€æ±‚è‡ªåŠ¨æ‰§è¡Œä»»åŠ¡ã€‚

ç¬¬å››æ­¥ï¼šæŠ¥å‘Šç”Ÿæˆ

ç”ŸæˆMarkdownæ ¼å¼çš„æŠ¥å‘Šï¼Œå¹¶å…è®¸ç”¨æˆ·é€šè¿‡è‡ªç„¶è¯­è¨€æŒ‡ä»¤è°ƒæ•´æ ¼å¼ã€‚

ç”±äºä»£ç é‡è¾ƒå¤§ï¼Œæˆ‘ä»¬å°†åˆ†æ–‡ä»¶å®ç°ã€‚ä»¥ä¸‹æ˜¯ä¿å§†çº§æ•™ç¨‹çš„æ­¥éª¤ï¼š

## 1. åˆ›å»ºé¡¹ç›®ç›®å½•ç»“æ„

paper_insight_expert/
â”‚   .env
â”‚   requirements.txt
â”‚   main.py
â”‚   agent.py
â”‚   tools/
â”‚   â”‚   __init__.py
â”‚   â”‚   arxiv_tools.py
â”‚   â”‚   pdf_tools.py
â”‚   â”‚   report_tools.py
â”‚   config/
â”‚   â”‚   __init__.py
â”‚   â”‚   settings.py

## 2. ç¯å¢ƒè®¾ç½®

åˆ›å»º`requirements.txt`ï¼š

anthropic
python-dotenv
arxiv
requests
beautifulsoup4
PyPDF2
pdfplumber
markdown

å®‰è£…ä¾èµ–ï¼š

pip install -r requirements.txt

åˆ›å»º`.env`æ–‡ä»¶ï¼Œå­˜æ”¾Anthropic API Keyï¼š

ANTHROPIC_API_KEY=ä½ çš„APIå¯†é’¥

## 3. æ„å»ºå·¥å…·

### 3.1 ArXivå·¥å…·ï¼ˆtools/arxiv_tools.pyï¼‰

import arxiv
import os
from datetime import datetime, timedelta

def search_papers(keywords, max_results=10, sort_by='relevance', sort_order='descending'):
    """
    åœ¨ArXivä¸Šæœç´¢è®ºæ–‡
    """
    client = arxiv.Client()
    
    # æ„å»ºæŸ¥è¯¢
    query = " AND ".join([f'"{keyword}"' for keyword in keywords])
    
    # æ’åºæ–¹å¼
    if sort_by == 'relevance':
        sort = arxiv.SortCriterion.Relevance
    elif sort_by == 'submittedDate':
        sort = arxiv.SortCriterion.SubmittedDate
    elif sort_by == 'lastUpdatedDate':
        sort = arxiv.SortCriterion.LastUpdatedDate
    else:
        sort = arxiv.SortCriterion.Relevance
        
    # æ’åºé¡ºåº
    if sort_order == 'ascending':
        sort_order = arxiv.SortOrder.Ascending
    else:
        sort_order = arxiv.SortOrder.Descending
        
    search = arxiv.Search(
        query=query,
        max_results=max_results,
        sort_by=sort,
        sort_order=sort_order
    )
    
    results = []
    for paper in client.results(search):
        results.append({
            'title': paper.title,
            'authors': [author.name for author in paper.authors],
            'summary': paper.summary,
            'published': paper.published.isoformat() if paper.published else None,
            'updated': paper.updated.isoformat() if paper.updated else None,
            'pdf_url': paper.pdf_url,
            'entry_id': paper.entry_id,
            'primary_category': paper.primary_category,
            'categories': paper.categories,
            'doi': paper.doi
        })
    
    return results

def get_recent_papers(keywords, days=7, max_results=10):
    """
    è·å–æœ€è¿‘å‡ å¤©å†…çš„è®ºæ–‡
    """
    client = arxiv.Client()
    query = " AND ".join([f'"{keyword}"' for keyword in keywords])
    since_date = (datetime.now() - timedelta(days=days)).date()
    
    search = arxiv.Search(
        query=query,
        max_results=max_results,
        sort_by=arxiv.SortCriterion.SubmittedDate,
        sort_order=arxiv.SortOrder.Descending
    )
    
    results = []
    for paper in client.results(search):
        if paper.published and paper.published.date() >= since_date:
            results.append({
                'title': paper.title,
                'authors': [author.name for author in paper.authors],
                'summary': paper.summary,
                'published': paper.published.isoformat(),
                'pdf_url': paper.pdf_url,
                'entry_id': paper.entry_id,
                'primary_category': paper.primary_category,
                'doi': paper.doi
            })
        if len(results) >= max_results:
            break
    
    return results

### 3.2 PDFå·¥å…·ï¼ˆtools/pdf_tools.pyï¼‰

import requests
import PyPDF2
import pdfplumber
import io
import os

def download_pdf(pdf_url, save_path=None):
    """
    ä¸‹è½½PDFæ–‡ä»¶
    """
    response = requests.get(pdf_url)
    if response.status_code == 200:
        if save_path:
            with open(save_path, 'wb') as f:
                f.write(response.content)
        return response.content
    else:
        return None

def extract_text_from_pdf(pdf_content):
    """
    ä»PDFå†…å®¹ä¸­æå–æ–‡æœ¬
    """
    try:
        # ä½¿ç”¨pdfplumberæå–æ–‡æœ¬
        with pdfplumber.open(io.BytesIO(pdf_content)) as pdf:
            text = ""
            for page in pdf.pages:
                page_text = page.extract_text()
                if page_text:
                    text += page_text + "\n"
        return text
    except Exception as e:
        print(f"Error extracting text with pdfplumber: {e}")
        # å›é€€åˆ°PyPDF2
        try:
            pdf_reader = PyPDF2.PdfReader(io.BytesIO(pdf_content))
            text = ""
            for page in pdf_reader.pages:
                text += page.extract_text() + "\n"
            return text
        except Exception as e2:
            print(f"Error extracting text with PyPDF2: {e2}")
            return None

def get_paper_info(pdf_content):
    """
    ä»PDFä¸­æå–è®ºæ–‡ä¿¡æ¯ï¼ˆæ ‡é¢˜ã€ä½œè€…ã€æ‘˜è¦ç­‰ï¼‰
    """
    text = extract_text_from_pdf(pdf_content)
    if text:
        # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤æ‚çš„è§£æé€»è¾‘ï¼Œæ¯”å¦‚ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–æ ‡é¢˜ã€ä½œè€…ç­‰
        # ç›®å‰åªæ˜¯è¿”å›å‰1000ä¸ªå­—ç¬¦ä½œä¸ºç¤ºä¾‹
        return text[:1000] + "..."
    else:
        return "æ— æ³•æå–æ–‡æœ¬"

### 3.3 æŠ¥å‘Šå·¥å…·ï¼ˆtools/report_tools.pyï¼‰

import os
from datetime import datetime

def generate_markdown_report(paper_info, output_path=None):
    """
    ç”ŸæˆMarkdownæ ¼å¼çš„æŠ¥å‘Š
    """
    # æŠ¥å‘Šæ ‡é¢˜
    report = "# è®ºæ–‡æ´å¯ŸæŠ¥å‘Š\n\n"
    report += f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
    
    # è®ºæ–‡åˆ—è¡¨
    report += "## è®ºæ–‡åˆ—è¡¨\n\n"
    for i, paper in enumerate(paper_info, 1):
        report += f"### {i}. {paper.get('title', 'æ— æ ‡é¢˜')}\n\n"
        report += f"- **ä½œè€…**: {', '.join(paper.get('authors', []))}\n"
        report += f"- **å‘å¸ƒæ—¶é—´**: {paper.get('published', 'æœªçŸ¥')}\n"
        report += f"- **åˆ†ç±»**: {paper.get('primary_category', 'æœªçŸ¥')}\n"
        if paper.get('doi'):
            report += f"- **DOI**: {paper.get('doi')}\n"
        report += f"- **æ‘˜è¦**: {paper.get('summary', 'æ— æ‘˜è¦')}\n"
        report += f"- **PDFé“¾æ¥**: [ä¸‹è½½]({paper.get('pdf_url')})\n\n"
        
        # å¦‚æœæœ‰æå–çš„æ–‡æœ¬ï¼Œå¯ä»¥æ·»åŠ ä¸€éƒ¨åˆ†
        if 'extracted_text' in paper:
            report += f"#### å†…å®¹é¢„è§ˆ:\n\n{paper['extracted_text'][:500]}...\n\n"
    
    # å¦‚æœæŒ‡å®šäº†è¾“å‡ºè·¯å¾„ï¼Œåˆ™å†™å…¥æ–‡ä»¶
    if output_path:
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(report)
    
    return report

def format_report(report, format_style='standard'):
    """
    æ ¹æ®æ ¼å¼é£æ ¼è°ƒæ•´æŠ¥å‘Š
    """
    if format_style == 'standard':
        return report
    elif format_style == 'compact':
        # ç´§å‡‘æ ¼å¼ï¼šå‡å°‘ç©ºè¡Œ
        lines = report.split('\n')
        compact_lines = [line for line in lines if line.strip() != '']
        return '\n'.join(compact_lines)
    elif format_style == 'detailed':
        # è¯¦ç»†æ ¼å¼ï¼šå¢åŠ æ›´å¤šä¿¡æ¯
        # è¿™é‡Œå¯ä»¥æ‰©å±•ï¼Œæ¯”å¦‚æ·»åŠ å›¾è¡¨ç­‰
        return report + "\n\n## åˆ†æ\n\nï¼ˆè¿™é‡Œæ˜¯è‡ªåŠ¨åˆ†æå†…å®¹ï¼‰"
    else:
        return report

## 4. è®¾è®¡æ™ºèƒ½ä½“ï¼ˆagent.pyï¼‰

import os
from anthropic import Anthropic
from dotenv import load_dotenv
from tools.arxiv_tools import search_papers, get_recent_papers
from tools.pdf_tools import download_pdf, extract_text_from_pdf
from tools.report_tools import generate_markdown_report, format_report

load_dotenv()

class PaperInsightAgent:
    def __init__(self):
        self.client = Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))
        self.tools = [
            {
                "name": "search_papers",
                "description": "åœ¨ArXivä¸Šæœç´¢è®ºæ–‡",
                "parameters": {
                    "keywords": "æœç´¢å…³é”®è¯åˆ—è¡¨",
                    "max_results": "æœ€å¤§ç»“æœæ•°ï¼Œé»˜è®¤10",
                    "sort_by": "æ’åºæ–¹å¼ï¼šrelevanceï¼ˆç›¸å…³æ€§ï¼‰ã€submittedDateï¼ˆæäº¤æ—¥æœŸï¼‰ã€lastUpdatedDateï¼ˆæœ€åæ›´æ–°æ—¥æœŸï¼‰",
                    "sort_order": "æ’åºé¡ºåºï¼šdescendingï¼ˆé™åºï¼‰ã€ascendingï¼ˆå‡åºï¼‰"
                }
            },
            {
                "name": "get_recent_papers",
                "description": "è·å–æœ€è¿‘å‡ å¤©å†…çš„è®ºæ–‡",
                "parameters": {
                    "keywords": "æœç´¢å…³é”®è¯åˆ—è¡¨",
                    "days": "æœ€è¿‘å‡ å¤©ï¼Œé»˜è®¤7",
                    "max_results": "æœ€å¤§ç»“æœæ•°ï¼Œé»˜è®¤10"
                }
            },
            {
                "name": "download_pdf",
                "description": "ä¸‹è½½PDFæ–‡ä»¶",
                "parameters": {
                    "pdf_url": "PDFçš„URL",
                    "save_path": "ä¿å­˜è·¯å¾„ï¼ˆå¯é€‰ï¼‰"
                }
            },
            {
                "name": "extract_text_from_pdf",
                "description": "ä»PDFå†…å®¹ä¸­æå–æ–‡æœ¬",
                "parameters": {
                    "pdf_content": "PDFçš„äºŒè¿›åˆ¶å†…å®¹"
                }
            },
            {
                "name": "generate_markdown_report",
                "description": "ç”ŸæˆMarkdownæ ¼å¼çš„æŠ¥å‘Š",
                "parameters": {
                    "paper_info": "è®ºæ–‡ä¿¡æ¯åˆ—è¡¨",
                    "output_path": "è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰"
                }
            },
            {
                "name": "format_report",
                "description": "æ ¹æ®æ ¼å¼é£æ ¼è°ƒæ•´æŠ¥å‘Š",
                "parameters": {
                    "report": "åŸå§‹æŠ¥å‘Šå†…å®¹",
                    "format_style": "æ ¼å¼é£æ ¼ï¼šstandardï¼ˆæ ‡å‡†ï¼‰ã€compactï¼ˆç´§å‡‘ï¼‰ã€detailedï¼ˆè¯¦ç»†ï¼‰"
                }
            }
        ]
    
    def run(self, user_input):
        # è¿™é‡Œæˆ‘ä»¬å°†å®ç°æ™ºèƒ½ä½“çš„å¾ªç¯
        # ç”±äºClaude Agent SDKçš„å…·ä½“ç”¨æ³•å¯èƒ½éšæ—¶é—´å˜åŒ–ï¼Œä»¥ä¸‹ä¸ºç®€åŒ–ç¤ºä¾‹
        
        message = self.client.messages.create(
            model="claude-3-5-sonnet-20240620",
            max_tokens=4096,
            tools=self.tools,
            system="ä½ æ˜¯ä¸€ä¸ªè®ºæ–‡æ´å¯Ÿä¸“å®¶ï¼Œèƒ½å¤Ÿå¸®åŠ©ç”¨æˆ·è¿½è¸ªæœ€æ–°è®ºæ–‡ã€ç”ŸæˆæŠ¥å‘Šå¹¶è°ƒæ•´æ ¼å¼ã€‚",
            messages=[
                {
                    "role": "user",
                    "content": user_input
                }
            ]
        )
        
        # å¤„ç†Claudeçš„å“åº”ï¼ŒåŒ…æ‹¬å·¥å…·è°ƒç”¨
        # è¿™é‡Œéœ€è¦æ ¹æ®Claudeçš„å“åº”æ¥è°ƒç”¨ç›¸åº”çš„å·¥å…·ï¼Œå¹¶å†æ¬¡å‘é€ç»“æœç»™Claude
        # ç”±äºä»£ç è¾ƒé•¿ï¼Œæˆ‘ä»¬ç®€åŒ–å¤„ç†
        
        return message.content

## 5. ä¸»ç¨‹åºï¼ˆmain.pyï¼‰

from agent import PaperInsightAgent

def main():
    agent = PaperInsightAgent()
    
    print("æ¬¢è¿ä½¿ç”¨è®ºæ–‡æ´å¯Ÿä¸“å®¶ï¼")
    print("è¯·è¾“å…¥æ‚¨çš„éœ€æ±‚ï¼Œä¾‹å¦‚ï¼š")
    print("- æœç´¢æœ€è¿‘ä¸€å‘¨å…³äº'å¤§è¯­è¨€æ¨¡å‹'çš„è®ºæ–‡")
    print("- è¿½è¸ª'å¤šæ¨¡æ€å­¦ä¹ 'çš„æœ€æ–°è¿›å±•ï¼Œå¹¶ç”ŸæˆæŠ¥å‘Š")
    print("- ä¸‹è½½'ç¥ç»ç½‘ç»œå‹ç¼©'çš„è®ºæ–‡å¹¶æå–æ‘˜è¦")
    
    while True:
        user_input = input("\næ‚¨çš„è¦æ±‚ï¼ˆè¾“å…¥'é€€å‡º'ç»“æŸï¼‰ï¼š")
        if user_input.lower() in ['é€€å‡º', 'exit', 'quit']:
            break
        
        response = agent.run(user_input)
        print("\nåŠ©æ‰‹å›å¤ï¼š")
        print(response)

if __name__ == "__main__":
    main()

## 6. ä½¿ç”¨ç¤ºä¾‹

è¿è¡Œç¨‹åºï¼š

python main.py

ç„¶åè¾“å…¥æ‚¨çš„éœ€æ±‚ï¼Œä¾‹å¦‚ï¼š

â€œæœç´¢æœ€è¿‘ä¸€å‘¨å…³äºâ€˜å¤§è¯­è¨€æ¨¡å‹â€™å’Œâ€˜æ¨ç†â€™çš„è®ºæ–‡ï¼Œç”Ÿæˆä¸€ä¸ªè¯¦ç»†æ ¼å¼çš„æŠ¥å‘Šï¼Œå¹¶ä¿å­˜ä¸ºreport.mdâ€

## 7. æ‰©å±•åŠŸèƒ½

- æ·»åŠ æ›´å¤šå­¦æœ¯æ•°æ®åº“ï¼ˆå¦‚ACLã€IEEEç­‰ï¼‰
    
- å®ç°è®ºæ–‡å¼•ç”¨è¿½è¸ª
    
- å¢åŠ è‡ªåŠ¨æ‘˜è¦åŠŸèƒ½
    
- æ”¯æŒLaTeXæŠ¥å‘Šç”Ÿæˆ
    
- æ·»åŠ å›¾è¡¨ç”Ÿæˆï¼ˆè®ºæ–‡è¶‹åŠ¿å›¾ç­‰ï¼‰
    

ç”±äºæ—¶é—´å…³ç³»ï¼Œä»¥ä¸Šä»£ç ä»…ä¸ºç¤ºä¾‹ï¼Œå¯èƒ½éœ€è¦æ ¹æ®å®é™…æƒ…å†µè¿›è¡Œè°ƒæ•´å’Œä¼˜åŒ–ã€‚ç‰¹åˆ«æ˜¯å·¥å…·è°ƒç”¨éƒ¨åˆ†ï¼Œéœ€è¦æ ¹æ®Claude Agent SDKçš„æœ€æ–°ç”¨æ³•æ¥å®ç°å¾ªç¯è°ƒç”¨ã€‚

å¸Œæœ›è¿™ä¸ªä¿å§†çº§æ•™ç¨‹èƒ½å¸®åŠ©ä½ æ„å»ºä¸€ä¸ªè®ºæ–‡æ´å¯Ÿä¸“å®¶ï¼

# è®ºæ–‡æ´å¯Ÿä¸“å®¶ï¼šå…¨è‡ªåŠ¨è®ºæ–‡è¿½è¸ªä¸æ’ç‰ˆç³»ç»Ÿï¼ˆä¿å§†çº§æ•™ç¨‹ï¼‰

æˆ‘å°†æ•™ä½ æ„å»ºä¸€ä¸ªèƒ½è‡ªåŠ¨è¿½è¸ªæœ€æ–°è®ºæ–‡ã€æ™ºèƒ½åˆ†æã€è‡ªåŠ¨æ’ç‰ˆå¹¶æ”¯æŒæ ¼å¼è°ƒèŠ‚çš„å®Œæ•´ç³»ç»Ÿã€‚è¿™ä¸ªç³»ç»Ÿå°†æ•´åˆå¤šä¸ªå­¦æœ¯èµ„æºï¼Œå®ç°çœŸæ­£çš„"ä¸€ç«™å¼"è®ºæ–‡ç®¡ç†ã€‚

## é¡¹ç›®æ¦‚è¿°

**è®ºæ–‡æ´å¯Ÿä¸“å®¶**å…·å¤‡ä»¥ä¸‹æ ¸å¿ƒèƒ½åŠ›ï¼š

- ğŸ“š **å¤šæºè¿½è¸ª**ï¼šåŒæ—¶ç›‘æ§ArXivã€PubMedã€ACL Anthologyç­‰ä¸»æµè®ºæ–‡å¹³å°
    
- ğŸ” **æ™ºèƒ½æ£€ç´¢**ï¼šåŸºäºä¸»é¢˜è¯­ä¹‰æœç´¢ï¼Œä¸ä»…ä»…æ˜¯å…³é”®è¯åŒ¹é…
    
- ğŸ“Š **è‡ªåŠ¨æº¯æº**ï¼šåˆ†æè®ºæ–‡çš„å¼•ç”¨å…³ç³»å’Œç ”ç©¶è„‰ç»œ
    
- ğŸ¨ **æ™ºèƒ½æ’ç‰ˆ**ï¼šè‡ªåŠ¨ç”Ÿæˆæ ¼å¼ä¼˜ç¾çš„æ–‡çŒ®ç»¼è¿°æŠ¥å‘Š
    
- âœï¸ **æ ¼å¼è°ƒèŠ‚**ï¼šæ”¯æŒå¤šç§å¼•ç”¨æ ¼å¼å’Œæ’ç‰ˆé£æ ¼
    

---

## ç¬¬ä¸€æ­¥ï¼šç¯å¢ƒæ­å»ºå’Œä¾èµ–å®‰è£…

### 1.1 åˆ›å»ºé¡¹ç›®ç»“æ„

`# åˆ›å»ºé¡¹ç›®æ–‡ä»¶å¤¹ mkdir paper-insight-expert cd paper-insight-expert  # åˆ›å»ºé¡¹ç›®ç»“æ„ mkdir -p src/{agents,tools,utils,config} mkdir -p data/{papers,cache,reports} mkdir -p tests  # åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ python -m venv venv # Windowsæ¿€æ´» venv\Scripts\activate # Mac/Linuxæ¿€æ´»   source venv/bin/activate`

### 1.2 å®‰è£…æ ¸å¿ƒä¾èµ–

åˆ›å»º `requirements.txt`ï¼š

`# æ ¸å¿ƒæ¡†æ¶ anthropic>=0.25.0 claude-agent-sdk>=1.0.0  # è®ºæ–‡æ£€ç´¢ arxiv>=2.1.0 scholarly>=0.5.0 semanticscholar>=1.0.0 crossrefapi>=1.0.0 requests>=2.31.0 beautifulsoup4>=4.12.0 selenium>=4.15.0  # æ•°æ®å¤„ç† pandas>=2.0.0 numpy>=1.24.0 scikit-learn>=1.3.0 networkx>=3.1  # æ–‡æ¡£å¤„ç† pypdf2>=3.0.0 pdfplumber>=0.10.0 python-docx>=1.0.0 markdown>=3.5.0 latex>=0.7.0  # å¯è§†åŒ–å’Œæ’ç‰ˆ matplotlib>=3.7.0 plotly>=5.17.0 seaborn>=0.13.0 jupyter>=1.0.0  # Webå’ŒAPI flask>=3.0.0 fastapi>=0.104.0 uvicorn>=0.24.0 websockets>=12.0  # å·¥å…·åº“ python-dotenv>=1.0.0 tqdm>=4.66.0 loguru>=0.7.0`

å®‰è£…ä¾èµ–ï¼š

`pip install -r requirements.txt`

---

## ç¬¬äºŒæ­¥ï¼šæ„å»ºæ ¸å¿ƒè®ºæ–‡æ£€ç´¢å¼•æ“

åˆ›å»º `src/tools/paper_fetcher.py`ï¼š

`# src/tools/paper_fetcher.py import arxiv import requests import json import time from datetime import datetime, timedelta from typing import List, Dict, Optional from loguru import logger import pandas as pd  class PaperFetcher:     """å¤šæºè®ºæ–‡æ£€ç´¢å¼•æ“"""          def __init__(self):         self.sources = {             'arxiv': self._fetch_arxiv,             'semantic_scholar': self._fetch_semantic_scholar,             'crossref': self._fetch_crossref         }          def search_papers(self,                       query: str,                       sources: List[str] = None,                      max_results: int = 50,                      days_back: int = 30) -> List[Dict]:         """å¤šæºè®ºæ–‡æœç´¢"""         if sources is None:             sources = ['arxiv', 'semantic_scholar']                  all_papers = []                  for source in sources:             if source in self.sources:                 try:                     logger.info(f"ä» {source} æœç´¢è®ºæ–‡: {query}")                     papers = self.sources[source](query, max_results, days_back)                     all_papers.extend(papers)                     time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«                 except Exception as e:                     logger.error(f"ä» {source} æœç´¢å¤±è´¥: {e}")                  # å»é‡å¹¶æ’åº         return self._deduplicate_and_sort(all_papers)          def _fetch_arxiv(self, query: str, max_results: int, days_back: int) -> List[Dict]:         """ä»ArXivè·å–è®ºæ–‡"""         client = arxiv.Client()                  # è®¡ç®—æ—¥æœŸèŒƒå›´         since_date = (datetime.now() - timedelta(days=days_back)).date()                  search = arxiv.Search(             query=query,             max_results=max_results,             sort_by=arxiv.SortCriterion.SubmittedDate         )                  papers = []         for result in client.results(search):             if result.published.date() >= since_date:                 paper = {                     'source': 'arxiv',                     'id': result.entry_id,                     'title': result.title,                     'authors': [author.name for author in result.authors],                     'abstract': result.summary,                     'published': result.published.isoformat(),                     'updated': result.updated.isoformat() if result.updated else None,                     'pdf_url': result.pdf_url,                     'primary_category': result.primary_category,                     'categories': result.categories,                     'doi': result.doi                 }                 papers.append(paper)                  return papers          def _fetch_semantic_scholar(self, query: str, max_results: int, days_back: int) -> List[Dict]:         """ä»Semantic Scholarè·å–è®ºæ–‡"""         url = "https://api.semanticscholar.org/graph/v1/paper/search"                  params = {             'query': query,             'limit': min(max_results, 100),             'fields': 'paperId,title,authors,abstract,venue,year,citationCount,referenceCount,url,publicationTypes,publicationDate,externalIds'         }                  try:             response = requests.get(url, params=params)             response.raise_for_status()             data = response.json()                          papers = []             for item in data.get('data', [])[:max_results]:                 # æ£€æŸ¥æ—¥æœŸ                 if 'publicationDate' in item:                     pub_date = datetime.fromisoformat(item['publicationDate'].replace('Z', '+00:00'))                     if (datetime.now() - pub_date).days > days_back:                         continue                                  paper = {                     'source': 'semantic_scholar',                     'id': item.get('paperId'),                     'title': item.get('title'),                     'authors': [author.get('name') for author in item.get('authors', [])],                     'abstract': item.get('abstract'),                     'published': item.get('publicationDate'),                     'venue': item.get('venue'),                     'citation_count': item.get('citationCount', 0),                     'reference_count': item.get('referenceCount', 0),                     'url': item.get('url'),                     'doi': item.get('externalIds', {}).get('DOI')                 }                 papers.append(paper)                          return papers                      except Exception as e:             logger.error(f"Semantic Scholar APIé”™è¯¯: {e}")             return []          def _fetch_crossref(self, query: str, max_results: int, days_back: int) -> List[Dict]:         """ä»Crossrefè·å–è®ºæ–‡"""         url = "https://api.crossref.org/works"                  params = {             'query': query,             'rows': min(max_results, 100),             'sort': 'published',             'order': 'desc'         }                  try:             response = requests.get(url, params=params)             response.raise_for_status()             data = response.json()                          papers = []             for item in data.get('message', {}).get('items', [])[:max_results]:                 paper = {                     'source': 'crossref',                     'id': item.get('DOI'),                     'title': ' '.join(item.get('title', [])),                     'authors': [f"{author.get('given', '')} {author.get('family', '')}".strip()                                 for author in item.get('author', [])],                     'abstract': item.get('abstract', ''),                     'published': item.get('created', {}).get('date-time'),                     'venue': item.get('container-title', [''])[0],                     'doi': item.get('DOI'),                     'url': item.get('URL')                 }                 papers.append(paper)                          return papers                      except Exception as e:             logger.error(f"Crossref APIé”™è¯¯: {e}")             return []          def _deduplicate_and_sort(self, papers: List[Dict]) -> List[Dict]:         """è®ºæ–‡å»é‡å’Œæ’åº"""         seen_titles = set()         unique_papers = []                  for paper in papers:             # ç®€å•çš„æ ‡é¢˜ç›¸ä¼¼åº¦å»é‡             title_lower = paper['title'].lower().strip() if paper['title'] else ''             if title_lower and title_lower not in seen_titles:                 seen_titles.add(title_lower)                 unique_papers.append(paper)                  # æŒ‰å‘å¸ƒæ—¶é—´æ’åºï¼ˆæ–°çš„åœ¨å‰ï¼‰         unique_papers.sort(key=lambda x: x.get('published', ''), reverse=True)                  return unique_papers  # è®ºæ–‡åˆ†æå·¥å…· class PaperAnalyzer:     """è®ºæ–‡å†…å®¹åˆ†æå™¨"""          def __init__(self):         self.analysis_cache = {}          def analyze_paper_trends(self, papers: List[Dict]) -> Dict:         """åˆ†æè®ºæ–‡è¶‹åŠ¿"""         if not papers:             return {}                  df = pd.DataFrame(papers)                  # åŸºæœ¬ç»Ÿè®¡         analysis = {             'total_papers': len(papers),             'sources': df['source'].value_counts().to_dict(),             'time_range': self._get_time_range(papers),             'top_authors': self._get_top_authors(papers),             'venue_distribution': self._get_venue_distribution(papers),             'citation_analysis': self._analyze_citations(papers)         }                  return analysis          def _get_time_range(self, papers: List[Dict]) -> Dict:         """è·å–æ—¶é—´èŒƒå›´"""         dates = [p.get('published') for p in papers if p.get('published')]         if dates:             return {'earliest': min(dates), 'latest': max(dates)}         return {}          def _get_top_authors(self, papers: List[Dict], top_n: int = 10) -> List[Dict]:         """è·å–é«˜äº§ä½œè€…"""         from collections import Counter         all_authors = []                  for paper in papers:             if paper.get('authors'):                 all_authors.extend(paper['authors'])                  author_counts = Counter(all_authors)         return [{'author': author, 'count': count}                  for author, count in author_counts.most_common(top_n)]          def _get_venue_distribution(self, papers: List[Dict]) -> Dict:         """è·å–å‘å¸ƒæ¸ é“åˆ†å¸ƒ"""         venues = [p.get('venue') for p in papers if p.get('venue')]         return dict(pd.Series(venues).value_counts().head(10))          def _analyze_citations(self, papers: List[Dict]) -> Dict:         """åˆ†æå¼•ç”¨æƒ…å†µ"""         citations = [p.get('citation_count', 0) for p in papers                      if p.get('citation_count') is not None]                  if citations:             return {                 'max_citations': max(citations),                 'avg_citations': sum(citations) / len(citations),                 'highly_cited': len([c for c in citations if c > 10])             }         return {}`

---

## ç¬¬ä¸‰æ­¥ï¼šæ„å»ºæ™ºèƒ½è®ºæ–‡åˆ†æAgent

åˆ›å»º `src/agents/paper_analysis_agent.py`ï¼š

`# src/agents/paper_analysis_agent.py import anthropic from typing import List, Dict, Any import json from datetime import datetime from ..tools.paper_fetcher import PaperFetcher, PaperAnalyzer  class PaperAnalysisAgent:     """è®ºæ–‡åˆ†ææ™ºèƒ½ä½“"""          def __init__(self, api_key: str):         self.client = anthropic.Anthropic(api_key=api_key)         self.fetcher = PaperFetcher()         self.analyzer = PaperAnalyzer()              def create_research_report(self,                               topic: str,                              format_style: str = "academic",                              include_trends: bool = True,                              max_papers: int = 100) -> Dict[str, Any]:         """åˆ›å»ºç ”ç©¶åˆ†ææŠ¥å‘Š"""                  # 1. æœç´¢ç›¸å…³è®ºæ–‡         papers = self.fetcher.search_papers(             query=topic,             max_results=max_papers,             days_back=180  # æœ€è¿‘6ä¸ªæœˆ         )                  # 2. åˆ†æè®ºæ–‡è¶‹åŠ¿         trends = self.analyzer.analyze_paper_trends(papers)                  # 3. ä½¿ç”¨Claudeç”Ÿæˆæ™ºèƒ½åˆ†ææŠ¥å‘Š         report_content = self._generate_ai_analysis(topic, papers, trends, format_style)                  return {             'topic': topic,             'generated_at': datetime.now().isoformat(),             'papers_analyzed': len(papers),             'trends': trends,             'report_content': report_content,             'papers': papers[:10]  # åªè¿”å›å‰10ç¯‡è®ºæ–‡çš„è¯¦ç»†ä¿¡æ¯         }          def _generate_ai_analysis(self,                              topic: str`